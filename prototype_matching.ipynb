{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "test_data = torch.load('data/test.pth')\n",
    "with open('data/constraint.pkl','rb') as f:\n",
    "    constraints = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "1. each p_i gives vote v_ijk to to each q_jk, where i indexes number of points, j number of prototypes and k number of points in prototype j\n",
    "\n",
    "2. if v_ijk > thresh1, add link e_ijk from q_jk to p_i with weight v_ijk\n",
    "\n",
    "3. for each q_jk get the max weight v_ijk and propagate it to relations r_kn it is part of by a message mr_jkn.\n",
    "\n",
    "4. For each rel r_kn sum the incomming messages mr_jkn. If the sum is bellow some threshold, discard this relation.\n",
    "\n",
    "5. For the relations that were above some threshold, send a message to their prototypes mp_kn.\n",
    "\n",
    "5. for each prototype k, sum the messages mp_kn. If its bellow some threshold, discard the prototype and also the relations with the points q_jk.\n",
    "\n",
    "6. for rels that remained find the best assignment for each rel, to get a score for this rel.\n",
    "\n",
    "7. propagate the scores to the prototypes to see if they are above some threshold. Discard the prototypes which are below some threshold\n",
    "\n",
    "8. for the remaining prototypes and relations, find the best match and get its score.\n",
    "\n",
    "9. choose the prototype with the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'3', '4'}, {'4', '6'}, {'5', '6'}, {'3', '6'}, {'6', '7'}, {'2', '6'}]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triang,triple = constraints['constraints'][0]\n",
    "all_c = triang + triple\n",
    "intersections_tuples = []\n",
    "intersections_points = []\n",
    "for c1 in all_c:\n",
    "    for c2 in all_c:\n",
    "        intersection = set(c1[0]).intersection(set(c2[0]))\n",
    "        if intersection in intersections_tuples + intersections_points:\n",
    "            continue\n",
    "        if len(intersection)==2:\n",
    "            intersections_tuples.append(intersection)\n",
    "        if len(intersection)==1:\n",
    "            intersections_points.append(intersection)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 92.7229232788086\n",
      "1 92.9883041381836\n",
      "2 91.63482666015625\n",
      "3 90.57596588134766\n",
      "4 88.09174346923828\n",
      "5 82.8647689819336\n",
      "6 90.45195007324219\n",
      "7 91.9672622680664\n",
      "8 90.08480072021484\n",
      "9 92.90975189208984\n",
      "10 95.73432922363281\n",
      "11 94.6197509765625\n",
      "12 91.07160186767578\n",
      "13 97.31986236572266\n",
      "14 89.64447784423828\n",
      "15 97.0658187866211\n",
      "16 94.54806518554688\n",
      "17 95.76974487304688\n",
      "18 93.71253967285156\n",
      "19 94.1404800415039\n",
      "20 84.6135482788086\n",
      "21 95.50888061523438\n",
      "22 95.49004364013672\n",
      "23 90.01053619384766\n",
      "24 90.48331451416016\n",
      "25 92.0970230102539\n",
      "26 91.99958801269531\n",
      "27 91.10466003417969\n",
      "28 91.28728485107422\n",
      "29 89.2131118774414\n",
      "30 91.75994873046875\n",
      "31 95.72162628173828\n",
      "32 96.72386932373047\n",
      "33 88.63195037841797\n",
      "34 93.23908996582031\n",
      "35 93.81224060058594\n",
      "36 92.87821197509766\n",
      "37 86.21138763427734\n",
      "38 90.26741027832031\n",
      "39 93.87928771972656\n",
      "40 91.44134521484375\n",
      "41 88.00782775878906\n",
      "42 95.69339752197266\n",
      "43 93.25434112548828\n",
      "44 94.42003631591797\n",
      "45 99.07709503173828\n",
      "46 90.42396545410156\n",
      "47 91.1737289428711\n",
      "48 95.36019134521484\n",
      "49 90.7667465209961\n",
      "50 95.95156860351562\n",
      "51 92.40948486328125\n",
      "52 95.46161651611328\n",
      "53 92.71546936035156\n",
      "54 88.77882385253906\n",
      "55 90.31304931640625\n",
      "56 87.29508972167969\n",
      "57 89.93763732910156\n",
      "58 97.99006652832031\n",
      "59 99.39043426513672\n",
      "60 91.66263580322266\n",
      "61 93.91458129882812\n",
      "62 96.80921173095703\n",
      "63 92.69837188720703\n",
      "64 90.26866912841797\n",
      "65 91.60005950927734\n",
      "66 93.53546905517578\n",
      "67 94.52267456054688\n",
      "68 87.90189361572266\n",
      "69 90.12596893310547\n",
      "70 90.19933319091797\n",
      "71 93.92017364501953\n",
      "72 93.37478637695312\n",
      "73 90.81330108642578\n",
      "74 93.66959381103516\n",
      "75 90.35079193115234\n",
      "76 91.87008666992188\n",
      "77 90.82672882080078\n",
      "78 96.74470520019531\n",
      "79 94.67467498779297\n",
      "80 93.24034118652344\n",
      "81 90.75614929199219\n",
      "82 85.65145874023438\n",
      "83 87.35358428955078\n",
      "84 93.15686798095703\n",
      "85 93.08916473388672\n",
      "86 95.09893035888672\n",
      "87 87.87602996826172\n",
      "88 97.76294708251953\n",
      "89 90.58929443359375\n",
      "90 89.97132110595703\n",
      "91 95.25203704833984\n",
      "92 88.00353240966797\n",
      "93 91.38057708740234\n",
      "94 92.27021789550781\n",
      "95 88.04927825927734\n",
      "96 90.97586822509766\n",
      "97 94.30323028564453\n",
      "98 94.81883239746094\n",
      "99 92.81382751464844\n",
      "100 93.79339599609375\n",
      "101 94.30213928222656\n",
      "102 91.20054626464844\n",
      "103 90.80873107910156\n",
      "104 92.50260925292969\n",
      "105 90.25861358642578\n",
      "106 96.03807067871094\n",
      "107 89.82145690917969\n",
      "108 90.87987518310547\n",
      "109 90.90357208251953\n",
      "110 93.56477355957031\n",
      "111 94.95429229736328\n",
      "112 86.13261413574219\n",
      "113 88.56602478027344\n",
      "114 89.61365509033203\n",
      "115 93.77686309814453\n",
      "116 93.93848419189453\n",
      "117 89.99197387695312\n",
      "118 96.48966979980469\n",
      "119 94.23382568359375\n",
      "120 94.42332458496094\n",
      "121 90.72054290771484\n",
      "122 89.1656723022461\n",
      "123 92.82550048828125\n",
      "124 97.29351806640625\n",
      "125 94.53129577636719\n",
      "126 90.08000946044922\n",
      "127 99.95198059082031\n",
      "128 89.88792419433594\n",
      "129 91.72296142578125\n",
      "130 90.96336364746094\n",
      "131 89.75553131103516\n",
      "132 89.34086608886719\n",
      "133 89.88838958740234\n",
      "134 89.27489471435547\n",
      "135 95.21582794189453\n",
      "136 95.67353057861328\n",
      "137 88.40544891357422\n",
      "138 89.63703918457031\n",
      "139 85.47846984863281\n",
      "140 92.53532409667969\n",
      "141 92.40897369384766\n",
      "142 93.33716583251953\n",
      "143 93.14643859863281\n",
      "144 95.8748550415039\n",
      "145 87.45280456542969\n",
      "146 93.67115020751953\n",
      "147 90.81273651123047\n",
      "148 89.61258697509766\n",
      "149 95.21509552001953\n",
      "150 93.01458740234375\n",
      "151 91.39928436279297\n",
      "152 93.12691497802734\n",
      "153 95.6986083984375\n",
      "154 87.43693542480469\n",
      "155 94.59902954101562\n",
      "156 90.09107208251953\n",
      "157 93.2486801147461\n",
      "158 88.34996032714844\n",
      "159 94.25695037841797\n",
      "160 91.27069854736328\n",
      "161 94.11305236816406\n",
      "162 89.98342895507812\n",
      "163 91.36267852783203\n",
      "164 89.601806640625\n",
      "165 91.15386199951172\n",
      "166 93.80162811279297\n",
      "167 91.111328125\n",
      "168 89.83348083496094\n",
      "169 94.33467102050781\n",
      "170 87.63912200927734\n",
      "171 103.05441284179688\n",
      "172 97.1219711303711\n",
      "173 94.71328735351562\n",
      "174 89.71729278564453\n",
      "175 92.38912200927734\n",
      "176 96.03341674804688\n",
      "177 92.5513687133789\n",
      "178 88.52759552001953\n",
      "179 96.69957733154297\n",
      "180 93.33661651611328\n",
      "181 96.10336303710938\n",
      "182 88.53409576416016\n",
      "183 93.20498657226562\n",
      "184 86.77102661132812\n",
      "185 90.14202880859375\n",
      "186 92.83869171142578\n",
      "187 96.58940124511719\n",
      "188 89.02544403076172\n",
      "189 94.65807342529297\n",
      "190 89.6956787109375\n",
      "191 86.86981964111328\n",
      "192 90.00570678710938\n",
      "193 96.04386901855469\n",
      "194 90.089599609375\n",
      "195 90.60967254638672\n",
      "196 94.0405502319336\n",
      "197 89.48693084716797\n",
      "198 92.96636962890625\n",
      "199 92.47898864746094\n",
      "200 92.18256378173828\n",
      "201 90.3531265258789\n",
      "202 99.08621215820312\n",
      "203 94.14545440673828\n",
      "204 97.08351135253906\n",
      "205 92.07810974121094\n",
      "206 97.82909393310547\n",
      "207 97.44906616210938\n",
      "208 98.03298950195312\n",
      "209 96.5427474975586\n",
      "210 97.37799835205078\n",
      "211 97.4846420288086\n",
      "212 98.62688446044922\n",
      "213 93.4649887084961\n",
      "214 96.38615417480469\n",
      "215 90.362548828125\n",
      "216 95.32821655273438\n",
      "217 96.04437255859375\n",
      "218 95.81961059570312\n",
      "219 91.61492919921875\n",
      "220 99.0283203125\n",
      "221 99.38442993164062\n",
      "222 100.64089965820312\n",
      "223 94.06593322753906\n",
      "224 92.29502868652344\n",
      "225 98.25148010253906\n",
      "226 95.1878662109375\n",
      "227 91.24132537841797\n",
      "228 92.36499786376953\n",
      "229 93.80094146728516\n",
      "230 94.45475006103516\n",
      "231 95.8727798461914\n",
      "232 97.37615966796875\n",
      "233 94.67424011230469\n",
      "234 98.14530944824219\n",
      "235 99.41493225097656\n",
      "236 89.8093032836914\n",
      "237 93.8465347290039\n",
      "238 94.27710723876953\n",
      "239 98.87493133544922\n",
      "240 102.50082397460938\n",
      "241 100.7941665649414\n",
      "242 91.13663482666016\n",
      "243 97.143798828125\n",
      "244 94.1787109375\n",
      "245 97.90357971191406\n",
      "246 95.35427856445312\n",
      "247 93.64752960205078\n",
      "248 95.35140991210938\n",
      "249 93.10823822021484\n",
      "250 94.46192169189453\n",
      "251 102.51382446289062\n",
      "252 96.04016876220703\n",
      "253 95.782958984375\n",
      "254 88.98540496826172\n",
      "255 90.65796661376953\n",
      "256 94.21448516845703\n",
      "257 98.44036865234375\n",
      "258 96.88221740722656\n",
      "259 93.71172332763672\n",
      "260 97.86833190917969\n",
      "261 95.73054504394531\n",
      "262 95.6737060546875\n",
      "263 98.06475830078125\n",
      "264 89.41751861572266\n",
      "265 97.71340942382812\n",
      "266 98.38018798828125\n",
      "267 93.0799789428711\n",
      "268 93.22440338134766\n",
      "269 93.37681579589844\n",
      "270 96.964599609375\n",
      "271 97.09274291992188\n",
      "272 96.97069549560547\n",
      "273 100.74818420410156\n",
      "274 91.11031341552734\n",
      "275 90.08402252197266\n",
      "276 99.69402313232422\n",
      "277 104.24311828613281\n",
      "278 94.61331939697266\n",
      "279 97.26887512207031\n",
      "280 95.95924377441406\n",
      "281 102.52701568603516\n",
      "282 92.53385925292969\n",
      "283 93.79267120361328\n",
      "284 94.0846939086914\n",
      "285 103.45803833007812\n",
      "286 97.68315124511719\n",
      "287 98.88280487060547\n",
      "288 95.37760925292969\n",
      "289 90.78401184082031\n",
      "290 92.78425598144531\n",
      "291 92.73995971679688\n",
      "292 97.0243148803711\n",
      "293 91.39191436767578\n",
      "294 98.34679412841797\n",
      "295 96.27603912353516\n",
      "296 92.9566650390625\n",
      "297 91.39879608154297\n",
      "298 94.9193115234375\n",
      "299 97.73604583740234\n",
      "300 103.85458374023438\n",
      "301 103.34434509277344\n",
      "302 94.35356903076172\n",
      "303 95.32858276367188\n",
      "304 98.51171875\n",
      "305 94.48594665527344\n",
      "306 94.21324157714844\n",
      "307 92.91366577148438\n",
      "308 89.09493255615234\n",
      "309 98.80854034423828\n",
      "310 97.2499008178711\n",
      "311 95.339111328125\n",
      "312 94.07152557373047\n",
      "313 93.3575210571289\n",
      "314 100.09646606445312\n",
      "315 100.66796875\n",
      "316 96.99728393554688\n",
      "317 101.86937713623047\n",
      "318 99.37648010253906\n",
      "319 94.06148529052734\n",
      "320 99.42030334472656\n",
      "321 97.20899200439453\n",
      "322 100.98942565917969\n",
      "323 99.12178039550781\n",
      "324 96.28697967529297\n",
      "325 92.58937072753906\n",
      "326 97.810546875\n",
      "327 99.1852798461914\n",
      "328 98.17266082763672\n",
      "329 101.89276885986328\n",
      "330 90.30455780029297\n",
      "331 95.43292999267578\n",
      "332 98.85151672363281\n",
      "333 90.85469818115234\n",
      "334 96.6832275390625\n",
      "335 97.41044616699219\n",
      "336 95.27249908447266\n",
      "337 98.68477630615234\n",
      "338 97.81206512451172\n",
      "339 93.58207702636719\n",
      "340 93.87910461425781\n",
      "341 92.55320739746094\n",
      "342 97.04420471191406\n",
      "343 96.59219360351562\n",
      "344 98.6838150024414\n",
      "345 99.86554718017578\n",
      "346 97.61616516113281\n",
      "347 93.77202606201172\n",
      "348 89.98286437988281\n",
      "349 98.25529479980469\n",
      "350 84.35140991210938\n",
      "351 100.16730499267578\n",
      "352 97.99486541748047\n",
      "353 91.88349914550781\n",
      "354 98.76342010498047\n",
      "355 102.74964904785156\n",
      "356 96.82313537597656\n",
      "357 91.82312774658203\n",
      "358 103.78897857666016\n",
      "359 95.53607940673828\n",
      "360 92.94448852539062\n",
      "361 93.00399017333984\n",
      "362 95.39244079589844\n",
      "363 102.00151062011719\n",
      "364 90.90018463134766\n",
      "365 99.20265197753906\n",
      "366 93.73246002197266\n",
      "367 102.42212677001953\n",
      "368 102.56916046142578\n",
      "369 96.1131362915039\n",
      "370 99.27970123291016\n",
      "371 99.29803466796875\n",
      "372 99.8971939086914\n",
      "373 92.38156127929688\n",
      "374 91.17607116699219\n",
      "375 95.24851989746094\n",
      "376 94.46112060546875\n",
      "377 96.51880645751953\n",
      "378 100.54905700683594\n",
      "379 95.82918548583984\n",
      "380 94.97332000732422\n",
      "381 93.30157470703125\n",
      "382 94.21319580078125\n",
      "383 96.24842071533203\n",
      "384 92.92762756347656\n",
      "385 99.87712097167969\n",
      "386 100.62068176269531\n",
      "387 97.76673126220703\n",
      "388 101.5205307006836\n",
      "389 100.10884094238281\n",
      "390 93.2304458618164\n",
      "391 90.31364440917969\n",
      "392 100.27124786376953\n",
      "393 101.28308868408203\n",
      "394 99.80683898925781\n",
      "395 101.13449096679688\n",
      "396 98.88136291503906\n",
      "397 95.29489135742188\n",
      "398 93.64916229248047\n",
      "399 99.4053726196289\n",
      "400 92.19351959228516\n",
      "401 95.80329132080078\n",
      "402 90.37589263916016\n",
      "403 87.50830841064453\n",
      "404 85.99005889892578\n",
      "405 97.451904296875\n",
      "406 91.76998138427734\n",
      "407 99.09527587890625\n",
      "408 88.85309600830078\n",
      "409 85.76079559326172\n",
      "410 87.77082061767578\n",
      "411 100.4595718383789\n",
      "412 89.886962890625\n",
      "413 91.6805648803711\n",
      "414 95.62752532958984\n",
      "415 91.91075134277344\n",
      "416 89.823486328125\n",
      "417 91.21251678466797\n",
      "418 87.68230438232422\n",
      "419 87.31614685058594\n",
      "420 90.53883361816406\n",
      "421 93.20370483398438\n",
      "422 94.14586639404297\n",
      "423 83.68597412109375\n",
      "424 97.8875732421875\n",
      "425 90.97005462646484\n",
      "426 100.36283111572266\n",
      "427 90.83428192138672\n",
      "428 96.45934295654297\n",
      "429 97.64556884765625\n",
      "430 86.4285659790039\n",
      "431 86.5486831665039\n",
      "432 91.15663146972656\n",
      "433 91.25244903564453\n",
      "434 90.2528305053711\n",
      "435 90.56009674072266\n",
      "436 93.63697814941406\n",
      "437 86.83438110351562\n",
      "438 96.07067108154297\n",
      "439 91.69132232666016\n",
      "440 94.84292602539062\n",
      "441 91.7670669555664\n",
      "442 98.60220336914062\n",
      "443 85.5235824584961\n",
      "444 90.48609924316406\n",
      "445 86.498046875\n",
      "446 92.20491790771484\n",
      "447 88.53256225585938\n",
      "448 91.59152221679688\n",
      "449 94.63809204101562\n",
      "450 94.00435638427734\n",
      "451 95.60053253173828\n",
      "452 92.845703125\n",
      "453 103.3863296508789\n",
      "454 97.03031921386719\n",
      "455 96.55406951904297\n",
      "456 92.51298522949219\n",
      "457 95.42179870605469\n",
      "458 90.2245101928711\n",
      "459 89.43549346923828\n",
      "460 89.225341796875\n",
      "461 96.37100219726562\n",
      "462 93.34121704101562\n",
      "463 90.69632720947266\n",
      "464 94.29700469970703\n",
      "465 90.18828582763672\n",
      "466 84.44733428955078\n",
      "467 83.95336151123047\n",
      "468 95.8543472290039\n",
      "469 94.9168930053711\n",
      "470 93.41322326660156\n",
      "471 90.51812744140625\n",
      "472 97.8382797241211\n",
      "473 96.5074234008789\n",
      "474 83.82556915283203\n",
      "475 83.05777740478516\n",
      "476 85.68860626220703\n",
      "477 86.73883819580078\n",
      "478 94.04261016845703\n",
      "479 90.2387924194336\n",
      "480 88.97171783447266\n",
      "481 87.134033203125\n",
      "482 91.60501861572266\n",
      "483 95.39374542236328\n",
      "484 87.45039367675781\n",
      "485 91.11844635009766\n",
      "486 88.27710723876953\n",
      "487 88.8787841796875\n",
      "488 95.87155151367188\n",
      "489 89.255615234375\n",
      "490 88.5963134765625\n",
      "491 88.45132446289062\n",
      "492 97.43787384033203\n",
      "493 95.28746795654297\n",
      "494 92.851318359375\n",
      "495 86.4455337524414\n",
      "496 104.22625732421875\n",
      "497 89.92544555664062\n",
      "498 93.38245391845703\n",
      "499 91.67115020751953\n",
      "500 96.32940673828125\n",
      "501 89.61750793457031\n",
      "502 81.86946105957031\n",
      "503 100.6365737915039\n",
      "504 91.65773010253906\n",
      "505 96.33731842041016\n",
      "506 89.44774627685547\n",
      "507 87.20234680175781\n",
      "508 96.99003601074219\n",
      "509 96.60517120361328\n",
      "510 91.51730346679688\n",
      "511 95.38774108886719\n",
      "512 91.90054321289062\n",
      "513 91.88810729980469\n",
      "514 93.48780059814453\n",
      "515 98.75643920898438\n",
      "516 91.87100219726562\n",
      "517 84.82826232910156\n",
      "518 87.43617248535156\n",
      "519 93.23766326904297\n",
      "520 95.28050994873047\n",
      "521 86.58116912841797\n",
      "522 88.47944641113281\n",
      "523 82.43611145019531\n",
      "524 94.41450500488281\n",
      "525 95.1942367553711\n",
      "526 88.78746795654297\n",
      "527 89.57148742675781\n",
      "528 90.40480041503906\n",
      "529 90.93016052246094\n",
      "530 95.14384460449219\n",
      "531 93.98042297363281\n",
      "532 96.14205169677734\n",
      "533 91.98228454589844\n",
      "534 89.19883728027344\n",
      "535 90.2271957397461\n",
      "536 91.0936050415039\n",
      "537 95.48008728027344\n",
      "538 88.30186462402344\n",
      "539 90.33425903320312\n",
      "540 93.19339752197266\n",
      "541 100.62140655517578\n",
      "542 85.26051330566406\n",
      "543 93.17964935302734\n",
      "544 87.42622375488281\n",
      "545 92.49329376220703\n",
      "546 81.60696411132812\n",
      "547 94.49126434326172\n",
      "548 91.09858703613281\n",
      "549 92.83321380615234\n",
      "550 89.3601303100586\n",
      "551 85.87712097167969\n",
      "552 89.74859619140625\n",
      "553 92.0555419921875\n",
      "554 88.64994812011719\n",
      "555 93.09695434570312\n",
      "556 90.83255004882812\n",
      "557 93.74982452392578\n",
      "558 101.18563842773438\n",
      "559 94.13927459716797\n",
      "560 88.93461608886719\n",
      "561 94.16997528076172\n",
      "562 92.99604034423828\n",
      "563 89.6282958984375\n",
      "564 95.5152359008789\n",
      "565 86.85334014892578\n",
      "566 88.54317474365234\n",
      "567 99.6956558227539\n",
      "568 94.63240051269531\n",
      "569 87.20221710205078\n",
      "570 97.19612884521484\n",
      "571 94.10401153564453\n",
      "572 94.41576385498047\n",
      "573 90.90313720703125\n",
      "574 93.6680908203125\n",
      "575 91.23832702636719\n",
      "576 94.26628875732422\n",
      "577 88.70266723632812\n",
      "578 93.77389526367188\n",
      "579 92.79844665527344\n",
      "580 92.71023559570312\n",
      "581 90.62456512451172\n",
      "582 90.96861267089844\n",
      "583 94.15803527832031\n",
      "584 88.41259765625\n",
      "585 85.33147430419922\n",
      "586 95.2362060546875\n",
      "587 94.59375\n",
      "588 95.07122039794922\n",
      "589 86.43152618408203\n",
      "590 93.99993896484375\n",
      "591 87.52359771728516\n",
      "592 86.72105407714844\n",
      "593 97.39360809326172\n",
      "594 100.43888092041016\n",
      "595 88.93374633789062\n",
      "596 92.85961151123047\n",
      "597 94.70718383789062\n",
      "598 92.16862487792969\n",
      "599 91.07217407226562\n",
      "600 69.40894317626953\n",
      "601 67.1148681640625\n",
      "602 64.10043334960938\n",
      "603 64.66260528564453\n",
      "604 69.277099609375\n",
      "605 70.12470245361328\n",
      "606 64.51750183105469\n",
      "607 68.89889526367188\n",
      "608 69.927734375\n",
      "609 70.85086822509766\n",
      "610 66.02424621582031\n",
      "611 66.20207977294922\n",
      "612 67.82830047607422\n",
      "613 65.04295349121094\n",
      "614 62.97237777709961\n",
      "615 68.37844848632812\n",
      "616 64.50165557861328\n",
      "617 68.89775848388672\n",
      "618 71.76107025146484\n",
      "619 68.2231674194336\n",
      "620 73.66355895996094\n",
      "621 69.1362533569336\n",
      "622 64.33490753173828\n",
      "623 67.7552490234375\n",
      "624 66.73808288574219\n",
      "625 68.6070556640625\n",
      "626 67.44215393066406\n",
      "627 66.10531616210938\n",
      "628 62.84577178955078\n",
      "629 67.09410858154297\n",
      "630 65.96739959716797\n",
      "631 68.14012908935547\n",
      "632 64.29405975341797\n",
      "633 69.02464294433594\n",
      "634 69.90502166748047\n",
      "635 60.73875045776367\n",
      "636 71.24182891845703\n",
      "637 74.04439544677734\n",
      "638 66.96937561035156\n",
      "639 70.1031723022461\n",
      "640 65.25347137451172\n",
      "641 67.81771087646484\n",
      "642 65.62879180908203\n",
      "643 74.30901336669922\n",
      "644 70.2708511352539\n",
      "645 67.37510681152344\n",
      "646 65.0716552734375\n",
      "647 69.34083557128906\n",
      "648 64.17324829101562\n",
      "649 68.95984649658203\n",
      "650 71.64940643310547\n",
      "651 64.4203872680664\n",
      "652 70.30904388427734\n",
      "653 66.75936889648438\n",
      "654 71.7601318359375\n",
      "655 72.9827651977539\n",
      "656 71.42695617675781\n",
      "657 64.90015411376953\n",
      "658 65.1075668334961\n",
      "659 65.62850952148438\n",
      "660 69.43415069580078\n",
      "661 67.66754150390625\n",
      "662 65.26966857910156\n",
      "663 64.54312896728516\n",
      "664 66.05101013183594\n",
      "665 67.22354125976562\n",
      "666 64.82147979736328\n",
      "667 69.78982543945312\n",
      "668 70.52167510986328\n",
      "669 64.21940612792969\n",
      "670 70.25508117675781\n",
      "671 72.60355377197266\n",
      "672 63.72336959838867\n",
      "673 63.44804763793945\n",
      "674 64.57376098632812\n",
      "675 65.45421600341797\n",
      "676 66.25550079345703\n",
      "677 72.69232177734375\n",
      "678 67.12625885009766\n",
      "679 65.052490234375\n",
      "680 67.25298309326172\n",
      "681 65.97013092041016\n",
      "682 68.59744262695312\n",
      "683 68.2416763305664\n",
      "684 62.756290435791016\n",
      "685 65.73223876953125\n",
      "686 67.08989715576172\n",
      "687 66.61835479736328\n",
      "688 68.20355987548828\n",
      "689 65.34477233886719\n",
      "690 65.56193542480469\n",
      "691 65.77702331542969\n",
      "692 68.77631378173828\n",
      "693 69.62664031982422\n",
      "694 72.41320037841797\n",
      "695 66.61568450927734\n",
      "696 61.786739349365234\n",
      "697 68.79602813720703\n",
      "698 65.77175903320312\n",
      "699 67.313720703125\n",
      "700 67.1174087524414\n",
      "701 72.9349594116211\n",
      "702 67.69586944580078\n",
      "703 68.7510757446289\n",
      "704 66.87167358398438\n",
      "705 64.85110473632812\n",
      "706 63.76712417602539\n",
      "707 70.0220718383789\n",
      "708 69.19296264648438\n",
      "709 67.72875213623047\n",
      "710 69.45738220214844\n",
      "711 64.71403503417969\n",
      "712 70.23787689208984\n",
      "713 64.42957305908203\n",
      "714 67.85877990722656\n",
      "715 66.2449722290039\n",
      "716 68.01817321777344\n",
      "717 69.19557189941406\n",
      "718 69.12622833251953\n",
      "719 63.27015686035156\n",
      "720 65.39388275146484\n",
      "721 70.7816162109375\n",
      "722 67.61788177490234\n",
      "723 67.66582489013672\n",
      "724 66.78363800048828\n",
      "725 70.26776123046875\n",
      "726 65.16841888427734\n",
      "727 71.93384552001953\n",
      "728 66.03173065185547\n",
      "729 69.24259185791016\n",
      "730 73.30587005615234\n",
      "731 75.11946868896484\n",
      "732 68.59783172607422\n",
      "733 62.05970764160156\n",
      "734 67.78669738769531\n",
      "735 72.92804718017578\n",
      "736 65.64364624023438\n",
      "737 69.12566375732422\n",
      "738 67.42535400390625\n",
      "739 68.36420440673828\n",
      "740 69.11920928955078\n",
      "741 68.73818969726562\n",
      "742 68.43019104003906\n",
      "743 68.77980041503906\n",
      "744 69.16136932373047\n",
      "745 68.90039825439453\n",
      "746 70.0982437133789\n",
      "747 69.23320770263672\n",
      "748 70.83118438720703\n",
      "749 65.84979248046875\n",
      "750 70.84624481201172\n",
      "751 65.75650024414062\n",
      "752 65.53094482421875\n",
      "753 66.01852416992188\n",
      "754 67.93456268310547\n",
      "755 65.15077209472656\n",
      "756 66.35777282714844\n",
      "757 68.61363220214844\n",
      "758 66.54969787597656\n",
      "759 68.47809600830078\n",
      "760 65.51988983154297\n",
      "761 68.15570068359375\n",
      "762 65.52135467529297\n",
      "763 68.98301696777344\n",
      "764 62.545188903808594\n",
      "765 66.5653076171875\n",
      "766 68.92294311523438\n",
      "767 62.51823043823242\n",
      "768 68.42752075195312\n",
      "769 67.4574203491211\n",
      "770 73.32001495361328\n",
      "771 76.21018981933594\n",
      "772 68.314453125\n",
      "773 66.83349609375\n",
      "774 69.84981536865234\n",
      "775 63.00510787963867\n",
      "776 69.0157241821289\n",
      "777 68.77769470214844\n",
      "778 66.23856353759766\n",
      "779 65.39407348632812\n",
      "780 72.45803833007812\n",
      "781 69.97888946533203\n",
      "782 64.34778594970703\n",
      "783 69.25831604003906\n",
      "784 72.0234146118164\n",
      "785 62.417442321777344\n",
      "786 70.82508850097656\n",
      "787 64.152099609375\n",
      "788 68.66747283935547\n",
      "789 68.06656646728516\n",
      "790 66.59597778320312\n",
      "791 70.19700622558594\n",
      "792 69.97858428955078\n",
      "793 67.5184555053711\n",
      "794 66.67245483398438\n",
      "795 66.3691635131836\n",
      "796 70.15446472167969\n",
      "797 72.60601043701172\n",
      "798 66.95077514648438\n",
      "799 69.9781723022461\n",
      "800 68.10218811035156\n",
      "801 67.1797103881836\n",
      "802 65.89837646484375\n",
      "803 61.99162292480469\n",
      "804 69.77184295654297\n",
      "805 74.72286224365234\n",
      "806 63.006813049316406\n",
      "807 63.90443420410156\n",
      "808 69.97056579589844\n",
      "809 75.24458312988281\n",
      "810 70.39451599121094\n",
      "811 68.29135131835938\n",
      "812 65.62837219238281\n",
      "813 70.17266845703125\n",
      "814 67.14647674560547\n",
      "815 68.83497619628906\n",
      "816 64.31935119628906\n",
      "817 72.09442901611328\n",
      "818 68.90155792236328\n",
      "819 69.09149169921875\n",
      "820 70.52161407470703\n",
      "821 68.41407775878906\n",
      "822 65.5242919921875\n",
      "823 67.04228973388672\n",
      "824 72.0037841796875\n",
      "825 66.63248443603516\n",
      "826 68.19816589355469\n",
      "827 66.09342193603516\n",
      "828 69.91266632080078\n",
      "829 72.66829681396484\n",
      "830 66.32492065429688\n",
      "831 67.6539535522461\n",
      "832 71.25475311279297\n",
      "833 70.60503387451172\n",
      "834 70.42469024658203\n",
      "835 69.20845794677734\n",
      "836 66.66461944580078\n",
      "837 66.12039947509766\n",
      "838 67.79105377197266\n",
      "839 68.89014434814453\n",
      "840 70.84890747070312\n",
      "841 66.26103973388672\n",
      "842 68.44461059570312\n",
      "843 75.70574188232422\n",
      "844 73.32196044921875\n",
      "845 71.24971771240234\n",
      "846 72.93356323242188\n",
      "847 65.62337493896484\n",
      "848 69.37166595458984\n",
      "849 64.78585815429688\n",
      "850 67.92587280273438\n",
      "851 65.315185546875\n",
      "852 66.93860626220703\n",
      "853 71.81891632080078\n",
      "854 68.88351440429688\n",
      "855 65.05377197265625\n",
      "856 64.55699157714844\n",
      "857 69.24685668945312\n",
      "858 67.2376480102539\n",
      "859 68.88274383544922\n",
      "860 69.0654296875\n",
      "861 66.29481506347656\n",
      "862 69.85176849365234\n",
      "863 70.68289947509766\n",
      "864 64.78416442871094\n",
      "865 72.34496307373047\n",
      "866 70.55168151855469\n",
      "867 66.22235870361328\n",
      "868 74.90593719482422\n",
      "869 73.74668884277344\n",
      "870 72.51933288574219\n",
      "871 66.27378845214844\n",
      "872 75.48371124267578\n",
      "873 70.43289947509766\n",
      "874 69.57816314697266\n",
      "875 67.2906723022461\n",
      "876 64.95685577392578\n",
      "877 67.92868041992188\n",
      "878 63.41370391845703\n",
      "879 69.78824615478516\n",
      "880 68.93136596679688\n",
      "881 67.44081115722656\n",
      "882 70.21998596191406\n",
      "883 71.99331665039062\n",
      "884 69.26760864257812\n",
      "885 65.97102355957031\n",
      "886 65.06022644042969\n",
      "887 68.72016143798828\n",
      "888 66.54232788085938\n",
      "889 70.4745101928711\n",
      "890 67.2510986328125\n",
      "891 65.93521118164062\n",
      "892 69.26426696777344\n",
      "893 66.426513671875\n",
      "894 66.09874725341797\n",
      "895 67.17138671875\n",
      "896 63.4078483581543\n",
      "897 67.07360076904297\n",
      "898 72.18843841552734\n",
      "899 70.28167724609375\n",
      "900 67.4725341796875\n",
      "901 67.02930450439453\n",
      "902 66.92091369628906\n",
      "903 74.130615234375\n",
      "904 67.97895050048828\n",
      "905 68.25799560546875\n",
      "906 62.180484771728516\n",
      "907 70.92440032958984\n",
      "908 64.11478424072266\n",
      "909 61.06242370605469\n",
      "910 62.42975616455078\n",
      "911 70.1683120727539\n",
      "912 68.2706298828125\n",
      "913 70.9284439086914\n",
      "914 72.11036682128906\n",
      "915 71.67215728759766\n",
      "916 66.6157455444336\n",
      "917 71.11402130126953\n",
      "918 63.24420166015625\n",
      "919 67.08843994140625\n",
      "920 67.00334930419922\n",
      "921 64.62247467041016\n",
      "922 66.45343780517578\n",
      "923 70.32492065429688\n",
      "924 66.34258270263672\n",
      "925 61.915103912353516\n",
      "926 65.50413513183594\n",
      "927 67.91847229003906\n",
      "928 69.92430114746094\n",
      "929 65.3809814453125\n",
      "930 73.65355682373047\n",
      "931 68.35581970214844\n",
      "932 63.54853057861328\n",
      "933 71.7221908569336\n",
      "934 65.4444351196289\n",
      "935 68.65552520751953\n",
      "936 72.13676452636719\n",
      "937 68.18727111816406\n",
      "938 64.1031265258789\n",
      "939 69.41230010986328\n",
      "940 63.16160202026367\n",
      "941 67.0045166015625\n",
      "942 61.74637222290039\n",
      "943 64.67867279052734\n",
      "944 64.13539123535156\n",
      "945 69.22692108154297\n",
      "946 65.17093658447266\n",
      "947 71.68885803222656\n",
      "948 70.13673400878906\n",
      "949 71.28434753417969\n",
      "950 61.16277313232422\n",
      "951 65.5975112915039\n",
      "952 65.37281036376953\n",
      "953 68.69518280029297\n",
      "954 71.03821563720703\n",
      "955 68.64569854736328\n",
      "956 65.6554946899414\n",
      "957 64.456787109375\n",
      "958 67.55091094970703\n",
      "959 63.96158981323242\n",
      "960 71.09098815917969\n",
      "961 66.15266418457031\n",
      "962 69.93487548828125\n",
      "963 77.1948471069336\n",
      "964 65.32516479492188\n",
      "965 67.4175033569336\n",
      "966 66.08490753173828\n",
      "967 67.7110824584961\n",
      "968 72.34196472167969\n",
      "969 66.80355072021484\n",
      "970 69.52587890625\n",
      "971 69.11070251464844\n",
      "972 65.68800354003906\n",
      "973 66.24473571777344\n",
      "974 68.67388916015625\n",
      "975 66.80390167236328\n",
      "976 66.09502410888672\n",
      "977 69.76124572753906\n",
      "978 73.08267974853516\n",
      "979 68.1427001953125\n",
      "980 65.58210754394531\n",
      "981 66.89624786376953\n",
      "982 65.00399017333984\n",
      "983 66.45580291748047\n",
      "984 75.50786590576172\n",
      "985 74.39371490478516\n",
      "986 64.8519058227539\n",
      "987 71.78116607666016\n",
      "988 67.26927185058594\n",
      "989 62.88755798339844\n",
      "990 69.65367889404297\n",
      "991 64.2592544555664\n",
      "992 74.0005874633789\n",
      "993 67.77621459960938\n",
      "994 68.94876861572266\n",
      "995 69.01800537109375\n",
      "996 70.10990905761719\n",
      "997 65.60205841064453\n",
      "998 71.65123748779297\n",
      "999 70.28398132324219\n"
     ]
    }
   ],
   "source": [
    "def get_average_feature_size_for_ex(ex):\n",
    "    cum = 0\n",
    "    for i in range(len(ex)):\n",
    "        cum += ex[i][1].norm()\n",
    "    cum /= len(ex)\n",
    "    return cum\n",
    "for i,ex in enumerate(test_data):\n",
    "    print(i,float(get_average_feature_size_for_ex(ex['points'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1024])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_constraint_features = []\n",
    "for cl in constraints['features']:\n",
    "    for f in cl:\n",
    "        flat_constraint_features.append(f)\n",
    "const_t = torch.stack(flat_constraint_features)\n",
    "const_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 50)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.linalg import block_diag\n",
    "def get_el_2_const_matrix_for_clas(cl):\n",
    "    triang,triple = constraints['constraints'][cl]\n",
    "    rows = []\n",
    "\n",
    "    for c in triang:\n",
    "        row = np.zeros(10)\n",
    "        for i in  c[0]:\n",
    "            row[int(i)-1] = 1\n",
    "        rows.append(row)\n",
    "    for c in triple:\n",
    "        row = np.zeros(10)\n",
    "        for i in  c[0]:\n",
    "            row[int(i)-1] = 1\n",
    "        rows.append(row)\n",
    "    return np.stack(rows)\n",
    "    \n",
    "def get_el_2_const_matrix():\n",
    "    blocks = []\n",
    "    for i in range(5):\n",
    "        block = get_el_2_const_matrix_for_clas(i)\n",
    "        blocks.append(block)\n",
    "    return block_diag(*blocks)\n",
    "get_el_2_const_matrix().shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 31)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_const_2_cl_matrix():\n",
    "    blocks = []\n",
    "    for i in range(5):\n",
    "        triang,triple = constraints['constraints'][i]\n",
    "        num_c = len(triang)+len(triple)\n",
    "        block = np.zeros((5,num_c))\n",
    "        block[i,:] = 1/num_c\n",
    "        blocks.append(block)\n",
    "    return np.concatenate(blocks,axis=1)\n",
    "get_const_2_cl_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_c = []\n",
    "all_c_per_class = []\n",
    "for i in range(5):\n",
    "    triang,triple = constraints['constraints'][i]\n",
    "    cs = triang+triple\n",
    "    all_c_for_class = []\n",
    "    for c in cs:\n",
    "        renamed = [int(s)-1 + 10*i for s in c[0] ]\n",
    "        all_c.append((renamed,c[1],c[2],i))\n",
    "        all_c_for_class.append(renamed)\n",
    "    all_c_per_class.append(all_c_for_class)\n",
    "len(all_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([2, 3, 0],\n",
       "  (1.0, 0.7071067811865476, 0.7071067811865476),\n",
       "  [[-1, 2], [1, 2], [0, 3]],\n",
       "  0),\n",
       " ([4, 5, 3],\n",
       "  (1.0, 0.4472135954999579, 0.6324555320336759),\n",
       "  [[3, 2], [0, 1], [1, 2]],\n",
       "  0),\n",
       " ([2, 5, 3],\n",
       "  (0.7071067811865476, 0.7071067811865476, 1.0),\n",
       "  [[-1, 2], [0, 1], [1, 2]],\n",
       "  0),\n",
       " ([1, 6, 5],\n",
       "  (1.0, 0.2773500981126146, 0.8770580193070293),\n",
       "  [[-3, 2], [0, 0], [0, 1]],\n",
       "  0),\n",
       " ([4, 6, 5],\n",
       "  (1.0, 0.2773500981126146, 0.8770580193070293),\n",
       "  [[3, 2], [0, 0], [0, 1]],\n",
       "  0),\n",
       " ([8, 9, 7],\n",
       "  (1.0, 0.7071067811865476, 0.7071067811865476),\n",
       "  [[-1, -2], [1, -2], [0, -1]],\n",
       "  0),\n",
       " ([2, 1, 5],\n",
       "  (0.6324555320336759, 1.0, 0.4472135954999579),\n",
       "  [[-1, 2], [-3, 2], [0, 1]],\n",
       "  0),\n",
       " ([5, 6, 7], (1.0, 1.0), [[0, 1], [0, 0], [0, -1]], 0),\n",
       " ([12, 11, 10],\n",
       "  (1.0, 0.7071067811865476, 0.7071067811865476),\n",
       "  [[-1, 1], [1, 1], [0, 2]],\n",
       "  1),\n",
       " ([12, 13, 11],\n",
       "  (0.7071067811865476, 0.7071067811865476, 1.0),\n",
       "  [[-1, 1], [0, 0], [1, 1]],\n",
       "  1),\n",
       " ([14, 15, 13],\n",
       "  (1.0, 0.7071067811865476, 0.7071067811865476),\n",
       "  [[-1, -1], [1, -1], [0, 0]],\n",
       "  1),\n",
       " ([17, 16, 15],\n",
       "  (0.7071067811865476, 1.0, 0.7071067811865476),\n",
       "  [[2, -2], [3, -1], [1, -1]],\n",
       "  1),\n",
       " ([18, 19, 14],\n",
       "  (0.7071067811865476, 0.7071067811865476, 1.0),\n",
       "  [[-3, -1], [-2, -2], [-1, -1]],\n",
       "  1),\n",
       " ([22, 29, 21],\n",
       "  (0.7071067811865475, 0.7071067811865475, 1.0),\n",
       "  [[1, 0], [0, 0], [0, 1]],\n",
       "  2),\n",
       " ([27, 29, 21],\n",
       "  (0.7071067811865475, 0.7071067811865475, 1.0),\n",
       "  [[-1, 0], [0, 0], [0, 1]],\n",
       "  2),\n",
       " ([20, 21, 29], (1.0, 1.0), [[0, 2], [0, 1], [0, 0]], 2),\n",
       " ([28, 24, 25], (1.0, 1.0), [[0, -1], [0, -2], [0, -3]], 2),\n",
       " ([37, 38, 30],\n",
       "  (0.7071067811865476, 0.7071067811865476, 1.0),\n",
       "  [[-1, 2], [0, 1], [1, 2]],\n",
       "  3),\n",
       " ([31, 38, 30],\n",
       "  (1.0, 0.7071067811865476, 0.7071067811865476),\n",
       "  [[2, 1], [0, 1], [1, 2]],\n",
       "  3),\n",
       " ([37, 36, 38],\n",
       "  (0.7071067811865476, 1.0, 0.7071067811865476),\n",
       "  [[-1, 2], [-2, 1], [0, 1]],\n",
       "  3),\n",
       " ([39, 34, 35],\n",
       "  (0.7071067811865476, 0.7071067811865476, 1.0),\n",
       "  [[0, 0], [-1, -1], [-2, 0]],\n",
       "  3),\n",
       " ([39, 33, 34],\n",
       "  (0.7071067811865476, 1.0, 0.7071067811865476),\n",
       "  [[0, 0], [1, -1], [-1, -1]],\n",
       "  3),\n",
       " ([39, 32, 33],\n",
       "  (1.0, 0.7071067811865476, 0.7071067811865476),\n",
       "  [[0, 0], [2, 0], [1, -1]],\n",
       "  3),\n",
       " ([44, 45, 41],\n",
       "  (0.7071067811865475, 0.7071067811865475, 1.0),\n",
       "  [[-1, 0], [0, 0], [0, 1]],\n",
       "  4),\n",
       " ([42, 45, 41],\n",
       "  (0.7071067811865475, 0.7071067811865475, 1.0),\n",
       "  [[1, 0], [0, 0], [0, 1]],\n",
       "  4),\n",
       " ([42, 43, 45],\n",
       "  (1.0, 0.7071067811865475, 0.7071067811865475),\n",
       "  [[1, 0], [0, -1], [0, 0]],\n",
       "  4),\n",
       " ([44, 43, 45],\n",
       "  (1.0, 0.7071067811865475, 0.7071067811865475),\n",
       "  [[-1, 0], [0, -1], [0, 0]],\n",
       "  4),\n",
       " ([48, 49, 43],\n",
       "  (1.0, 0.7071067811865476, 0.7071067811865476),\n",
       "  [[-1, -2], [1, -2], [0, -1]],\n",
       "  4),\n",
       " ([40, 41, 45], (1.0, 1.0), [[0, 2], [0, 1], [0, 0]], 4),\n",
       " ([46, 44, 45], (1.0, 1.0), [[-2, 0], [-1, 0], [0, 0]], 4),\n",
       " ([47, 42, 43], (0.7071067811865475, 1.0), [[2, 0], [1, 0], [0, -1]], 4)]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[({2, 3}, (0, 2)),\n",
       "  ({3, 5}, (1, 2)),\n",
       "  ({4, 5}, (1, 4)),\n",
       "  ({2, 5}, (2, 6)),\n",
       "  ({5, 6}, (3, 4)),\n",
       "  ({1, 5}, (3, 6))],\n",
       " [({11, 12}, (8, 9))],\n",
       " [({21, 29}, (13, 14))],\n",
       " [({30, 38}, (17, 18)),\n",
       "  ({37, 38}, (17, 19)),\n",
       "  ({34, 39}, (20, 21)),\n",
       "  ({33, 39}, (21, 22))],\n",
       " [({41, 45}, (23, 24)),\n",
       "  ({44, 45}, (23, 26)),\n",
       "  ({42, 45}, (24, 25)),\n",
       "  ({43, 45}, (25, 26)),\n",
       "  ({42, 43}, (25, 30))]]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def intersections_per_class(all_c,cc):\n",
    "    intersections_tuples = []\n",
    "    intersections_points = []\n",
    "    matched = []\n",
    "    for i1,c1 in enumerate(all_c):\n",
    "        for i2,c2 in enumerate(all_c):\n",
    "            intersection = set(c1).intersection(set(c2))\n",
    "            if intersection in matched:\n",
    "                continue\n",
    "            if len(intersection)==2:\n",
    "                intersections_tuples.append((intersection,(i1+cc,i2+cc)))\n",
    "                matched.append(intersection)\n",
    "            if len(intersection)==1:\n",
    "                intersections_points.append((intersection,(i1+cc,i2+cc)))\n",
    "                matched.append(intersection)\n",
    "    return (intersections_tuples,intersections_points)\n",
    "all_intersections1 = []\n",
    "all_intersections2 = []\n",
    "cc=0\n",
    "for i in range(5):\n",
    "    i2,i1 = intersections_per_class(all_c_per_class[i],cc)\n",
    "    all_intersections2.append((i2))\n",
    "    all_intersections1.append((i1))\n",
    "    cc+=len(all_c_per_class[i])\n",
    "all_intersections2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [1, 6],\n",
       "             1: [3],\n",
       "             5: [7],\n",
       "             9: [10],\n",
       "             10: [11, 12],\n",
       "             18: [19],\n",
       "             20: [22],\n",
       "             23: [25],\n",
       "             24: [30],\n",
       "             25: [27]})"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "c_neigh_1 = defaultdict(list) \n",
    "for a in all_intersections1:\n",
    "    for isc in a:\n",
    "         c_neigh_1[isc[1][0]].append(isc[1][1])\n",
    "c_neigh_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [2],\n",
       "             1: [2, 4],\n",
       "             2: [6],\n",
       "             3: [4, 6],\n",
       "             8: [9],\n",
       "             13: [14],\n",
       "             17: [18, 19],\n",
       "             20: [21],\n",
       "             21: [22],\n",
       "             23: [24, 26],\n",
       "             24: [25],\n",
       "             25: [26, 30]})"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_neigh_2 = defaultdict(list) \n",
    "for a in all_intersections2:\n",
    "    for isc in a:\n",
    "         c_neigh_2[isc[1][0]].append(isc[1][1])\n",
    "c_neigh_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(['5', '6', '2'],\n",
       "   (0.7071067811865475, 0.7071067811865475, 1.0),\n",
       "   [[-1, 0], [0, 0], [0, 1]]),\n",
       "  (['3', '6', '2'],\n",
       "   (0.7071067811865475, 0.7071067811865475, 1.0),\n",
       "   [[1, 0], [0, 0], [0, 1]]),\n",
       "  (['3', '4', '6'],\n",
       "   (1.0, 0.7071067811865475, 0.7071067811865475),\n",
       "   [[1, 0], [0, -1], [0, 0]]),\n",
       "  (['5', '4', '6'],\n",
       "   (1.0, 0.7071067811865475, 0.7071067811865475),\n",
       "   [[-1, 0], [0, -1], [0, 0]]),\n",
       "  (['9', '10', '4'],\n",
       "   (1.0, 0.7071067811865476, 0.7071067811865476),\n",
       "   [[-1, -2], [1, -2], [0, -1]])],\n",
       " [([1, 2, 6], (1.0, 1.0), [[0, 2], [0, 1], [0, 0]]),\n",
       "  ([7, 5, 6], (1.0, 1.0), [[-2, 0], [-1, 0], [0, 0]]),\n",
       "  ([8, 3, 4], (0.7071067811865475, 1.0), [[2, 0], [1, 0], [0, -1]])])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints['constraints'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 50])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el_to_const.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_to_const = torch.from_numpy(get_el_2_const_matrix().astype('float32')) \n",
    "const_to_class = torch.from_numpy(get_const_2_cl_matrix().astype('float32'))\n",
    "clas_to_const = const_to_class.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos,features,labels = list(zip(*test_data[0]['points']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = torch.stack(features).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = torch.stack(features).T\n",
    "feature_matrix_norm = feature_matrix / feature_matrix.norm(dim=0)\n",
    "const_t_norm = (const_t.transpose(0,1)/const_t.norm(dim=1)).transpose(0,1)\n",
    "res = torch.mm(const_t_norm, feature_matrix_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 0.3\n",
    "t2 = 0.8\n",
    "t3 = 0.8\n",
    "t4 = 0.3\n",
    "t5 = 0.3\n",
    "t6 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def find_the_match(best10_per_const):\n",
    "    pass\n",
    "\n",
    "def print_block(title,x):\n",
    "    print('='*5, title)\n",
    "    print(x)\n",
    "    print('-'*20)\n",
    "mismatches = []\n",
    "def get_class(features,pos,debug=False):\n",
    "    feature_matrix = torch.stack(features).T\n",
    "    feature_matrix_norm = feature_matrix / feature_matrix.norm(dim=0)\n",
    "    const_t_norm = (const_t.transpose(0,1)/const_t.norm(dim=1)).transpose(0,1)\n",
    "    res = torch.mm(const_t_norm, feature_matrix_norm)\n",
    "\n",
    "    res_masked = res.masked_fill(res < t1,0)\n",
    "    #torch.sort(res_masked,dim=1,descending=True)\n",
    "    max_vals,max_vals_ix = res_masked.max(dim=1)\n",
    "    if debug:\n",
    "        print_block('max vals',max_vals)\n",
    "    cons_score = torch.mv(el_to_const,max_vals)\n",
    "    if debug:\n",
    "        print_block('cons_score',cons_score)\n",
    "    cons_score_masked = cons_score.masked_fill(cons_score < t2,0)\n",
    "    if debug:\n",
    "        print_block('cons_score_masked',cons_score_masked)\n",
    "    clas_score = torch.mv(const_to_class,cons_score_masked)\n",
    "    if debug:\n",
    "        print_block('clas_score',clas_score)\n",
    "    clas_score_masked = (clas_score > t3).float()\n",
    "    #return clas_score_masked\n",
    "    const_back = torch.mv(clas_to_const,clas_score_masked)\n",
    "    filtered_const = const_back * cons_score_masked\n",
    "    if debug:\n",
    "        print_block('filtered_const',filtered_const)\n",
    "    res_bool = res > t1\n",
    "    counts = torch.sum(res_bool.float(),dim=1).long() \n",
    "    sorted_res_masked,ixs = torch.sort(res_masked,dim=1,descending=True)\n",
    "\n",
    "    choosed_points = []\n",
    "    for i,c in enumerate(counts):\n",
    "        scrs = sorted_res_masked[i][:c]\n",
    "        ps = np.array(pos)[ixs[i][:c].numpy()]\n",
    "        choosed_points.append((scrs,ps))\n",
    "    choosed_points_np = np.array(choosed_points)\n",
    "    idx = 0\n",
    "    def calcDist(p1,p2):\n",
    "        x1,y1 = p1\n",
    "        x2,y2 = p2\n",
    "        dist = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "        return dist\n",
    "\n",
    "    def get_score(p1,p2,p3,c,s1,s2,s3):\n",
    "        ab = calcDist(p1,p2)\n",
    "        bc = calcDist(p2,p3)\n",
    "        ca = calcDist(p3,p1)\n",
    "        max_d = np.max([ab,bc,ca])\n",
    "        abn,bcn,can = ab/max_d,bc/max_d,ca/max_d\n",
    "\n",
    "        mismatch = (1-(abn - c[0]))*(1-(bcn - c[1]))*(s1+s2+s3) \n",
    "        if len(c)==3:\n",
    "            mismatch *= (1-(can - c[2]))\n",
    "\n",
    "        if mismatch < t6:\n",
    "            return 0\n",
    "        else: return mismatch\n",
    "\n",
    "    def get_best_score(candidates,c):\n",
    "        # rewrite for better effectivity\n",
    "        old_score = 0\n",
    "        best10 = []\n",
    "        for i1,_ in enumerate(candidates[0][0]):\n",
    "            s1,p1 = candidates[0][0][i1],candidates[0][1][i1]\n",
    "            for i2,_ in enumerate(candidates[1][0]):\n",
    "                s2,p2 = candidates[1][0][i2],candidates[1][1][i2]\n",
    "                for i3,_ in enumerate(candidates[2][0]):\n",
    "                    s3,p3 = candidates[2][0][i3],candidates[2][1][i3]\n",
    "                    score = get_score(p1,p2,p3,c,s1,s2,s3)\n",
    "                    if score > old_score:\n",
    "                        old_score = score\n",
    "        return old_score,best10\n",
    "\n",
    "\n",
    "    def get_score_for_constraint(c):\n",
    "        candidates = choosed_points_np[c[0]]\n",
    "        scre,best10 = get_best_score(candidates,c[1])\n",
    "        return scre,best10\n",
    "\n",
    "    best_c_scores = torch.zeros(len(filtered_const))\n",
    "    best10_per_const = []\n",
    "    for i,v in enumerate(filtered_const):\n",
    "        if v > 0:\n",
    "            scre,best10 = get_score_for_constraint(all_c[i])\n",
    "            best10_per_const.append(best10)\n",
    "            best_c_scores[i] = scre\n",
    "        else:\n",
    "            best10_per_const.append([])\n",
    "    if debug:\n",
    "        print_block('best_c_scores',best_c_scores)\n",
    "    best_c_scores_masked = cons_score.masked_fill(best_c_scores < t4,0)\n",
    "    if debug:\n",
    "        print_block('best_c_scores_masked',best_c_scores_masked)\n",
    "    clas_score_2 = torch.mv(const_to_class,best_c_scores_masked)\n",
    "    if debug:\n",
    "        print_block('clas_score_2',clas_score_2)\n",
    "    #clas_score_2_masked = (clas_score_2 > t5).float()\n",
    "    find_the_match(best10_per_const)\n",
    "    return clas_score_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9305)\n",
      "tensor(1.5905)\n",
      "tensor(1.2248)\n",
      "tensor(1.3728)\n",
      "tensor(1.4085)\n",
      "tensor(1.1562)\n",
      "tensor(1.5289)\n",
      "tensor(1.6187)\n",
      "tensor(1.6381)\n",
      "tensor(1.1944)\n",
      "tensor(0.9018)\n",
      "tensor(1.5274)\n",
      "tensor(1.0478)\n",
      "tensor(1.3840)\n",
      "tensor(1.2700)\n",
      "tensor(0.7161)\n",
      "tensor(1.5023)\n",
      "tensor(1.2497)\n",
      "tensor(1.4964)\n",
      "tensor(1.2916)\n",
      "tensor(1.2633)\n",
      "tensor(1.5010)\n",
      "tensor(1.4409)\n",
      "tensor(0.9046)\n",
      "tensor(1.5002)\n",
      "tensor(1.3172)\n",
      "tensor(1.4871)\n",
      "tensor(1.6882)\n",
      "tensor(1.2790)\n",
      "tensor(1.3856)\n",
      "tensor(1.5554)\n",
      "tensor(1.3540)\n",
      "tensor(1.4628)\n",
      "tensor(1.3927)\n",
      "tensor(1.3492)\n",
      "tensor(1.1112)\n",
      "tensor(1.0069)\n",
      "tensor(0.9585)\n",
      "tensor(0.9741)\n",
      "tensor(1.2876)\n",
      "tensor(1.4647)\n",
      "tensor(1.2076)\n",
      "tensor(1.3187)\n",
      "tensor(1.3516)\n",
      "tensor(1.2862)\n",
      "tensor(1.4962)\n",
      "tensor(1.3090)\n",
      "tensor(1.5052)\n",
      "tensor(1.1181)\n",
      "tensor(1.2611)\n",
      "tensor(1.3547)\n",
      "tensor(1.3818)\n",
      "tensor(1.3012)\n",
      "tensor(1.1579)\n",
      "tensor(1.2597)\n",
      "tensor(1.1459)\n",
      "tensor(1.1162)\n",
      "tensor(1.0664)\n",
      "tensor(1.0342)\n",
      "tensor(nan)\n",
      "tensor(1.4692)\n",
      "tensor(0.8381)\n",
      "tensor(1.3941)\n",
      "tensor(1.3658)\n",
      "tensor(1.4148)\n",
      "tensor(1.4076)\n",
      "tensor(1.0069)\n",
      "tensor(1.1542)\n",
      "tensor(1.5293)\n",
      "tensor(1.1047)\n",
      "tensor(1.4062)\n",
      "tensor(0.7834)\n",
      "tensor(1.2572)\n",
      "tensor(1.3791)\n",
      "tensor(1.3246)\n",
      "tensor(1.3811)\n",
      "tensor(0.6447)\n",
      "tensor(1.3685)\n",
      "tensor(1.1283)\n",
      "tensor(1.2974)\n",
      "tensor(1.3797)\n",
      "tensor(1.1942)\n",
      "tensor(1.2838)\n",
      "tensor(1.3266)\n",
      "tensor(1.3029)\n",
      "tensor(1.3659)\n",
      "tensor(1.2147)\n",
      "tensor(1.4444)\n",
      "tensor(1.4383)\n",
      "tensor(1.2285)\n",
      "tensor(1.4631)\n",
      "tensor(0.8779)\n",
      "tensor(1.4187)\n",
      "tensor(1.4538)\n",
      "tensor(1.3624)\n",
      "tensor(1.4833)\n",
      "tensor(1.2142)\n",
      "tensor(0.8833)\n",
      "tensor(1.4600)\n",
      "tensor(1.0763)\n",
      "tensor(1.4360)\n",
      "tensor(0.9838)\n",
      "tensor(1.2775)\n",
      "tensor(1.4483)\n",
      "tensor(1.2866)\n",
      "tensor(1.2169)\n",
      "tensor(1.0812)\n",
      "tensor(1.2959)\n",
      "tensor(0.7842)\n",
      "tensor(1.1622)\n",
      "tensor(1.3460)\n",
      "tensor(0.6640)\n",
      "tensor(1.2636)\n",
      "tensor(1.3763)\n",
      "tensor(1.2373)\n",
      "tensor(1.3885)\n",
      "tensor(0.9386)\n",
      "tensor(1.3827)\n",
      "tensor(1.4053)\n",
      "tensor(1.2083)\n",
      "tensor(1.2279)\n",
      "tensor(1.6230)\n",
      "tensor(1.0506)\n",
      "tensor(1.2531)\n",
      "tensor(1.4448)\n",
      "tensor(1.6928)\n",
      "tensor(1.1652)\n",
      "tensor(1.2998)\n",
      "tensor(1.0871)\n",
      "tensor(1.3443)\n",
      "tensor(0.6730)\n",
      "tensor(0.8658)\n",
      "tensor(1.4669)\n",
      "tensor(0.9613)\n",
      "tensor(1.3226)\n",
      "tensor(1.5097)\n",
      "tensor(1.3051)\n",
      "tensor(1.0697)\n",
      "tensor(1.4138)\n",
      "tensor(1.1337)\n",
      "tensor(1.3783)\n",
      "tensor(1.2111)\n",
      "tensor(1.1454)\n",
      "tensor(1.0359)\n",
      "tensor(1.3981)\n",
      "tensor(1.1577)\n",
      "tensor(1.2610)\n",
      "tensor(1.3726)\n",
      "tensor(1.2877)\n",
      "tensor(1.4489)\n",
      "tensor(1.2161)\n",
      "tensor(1.4518)\n",
      "tensor(1.2510)\n",
      "tensor(0.8034)\n",
      "tensor(1.1228)\n",
      "tensor(1.2582)\n",
      "tensor(1.4228)\n",
      "tensor(0.8859)\n",
      "tensor(1.2814)\n",
      "tensor(1.2999)\n",
      "tensor(1.2675)\n",
      "tensor(1.3870)\n",
      "tensor(1.1353)\n",
      "tensor(1.2151)\n",
      "tensor(0.9850)\n",
      "tensor(1.1898)\n",
      "tensor(1.1594)\n",
      "tensor(1.1120)\n",
      "tensor(1.2179)\n",
      "tensor(0.8341)\n",
      "tensor(1.4618)\n",
      "tensor(1.4622)\n",
      "tensor(1.3734)\n",
      "tensor(1.3909)\n",
      "tensor(1.3283)\n",
      "tensor(1.0764)\n",
      "tensor(1.2864)\n",
      "tensor(1.3600)\n",
      "tensor(1.3516)\n",
      "tensor(1.3113)\n",
      "tensor(1.3620)\n",
      "tensor(1.3984)\n",
      "tensor(0.8666)\n",
      "tensor(1.2140)\n",
      "tensor(1.3688)\n",
      "tensor(1.3620)\n",
      "tensor(1.3898)\n",
      "tensor(1.4303)\n",
      "tensor(1.1889)\n",
      "tensor(1.4087)\n",
      "tensor(1.2847)\n",
      "tensor(1.3722)\n",
      "tensor(1.3298)\n",
      "tensor(1.3972)\n",
      "tensor(1.2695)\n",
      "tensor(1.3132)\n",
      "tensor(1.1714)\n",
      "tensor(0.7207)\n",
      "tensor(1.3349)\n",
      "tensor(1.4208)\n",
      "tensor(1.2870)\n",
      "tensor(1.3963)\n",
      "tensor(1.2773)\n",
      "tensor(1.0362)\n",
      "tensor(1.2032)\n",
      "tensor(1.3615)\n",
      "tensor(1.2548)\n",
      "tensor(1.3349)\n",
      "tensor(1.2407)\n",
      "tensor(1.3587)\n",
      "tensor(1.2823)\n",
      "tensor(0.6840)\n",
      "tensor(1.4745)\n",
      "tensor(0.9251)\n",
      "tensor(1.1489)\n",
      "tensor(0.6800)\n",
      "tensor(1.0176)\n",
      "tensor(1.2113)\n",
      "tensor(1.1926)\n",
      "tensor(0.8918)\n",
      "tensor(1.1974)\n",
      "tensor(0.7122)\n",
      "tensor(1.3914)\n",
      "tensor(0.8548)\n",
      "tensor(0.6739)\n",
      "tensor(1.4540)\n",
      "tensor(1.0619)\n",
      "tensor(1.0083)\n",
      "tensor(1.2459)\n",
      "tensor(0.8868)\n",
      "tensor(0.7828)\n",
      "tensor(1.3007)\n",
      "tensor(1.1884)\n",
      "tensor(0.8743)\n",
      "tensor(1.1388)\n",
      "tensor(0.9421)\n",
      "tensor(0.9800)\n",
      "tensor(0.9217)\n",
      "tensor(1.2875)\n",
      "tensor(1.0646)\n",
      "tensor(1.3021)\n",
      "tensor(1.1723)\n",
      "tensor(1.0037)\n",
      "tensor(0.9699)\n",
      "tensor(1.2705)\n",
      "tensor(1.0602)\n",
      "tensor(1.1832)\n",
      "tensor(1.1137)\n",
      "tensor(0.9516)\n",
      "tensor(0.8940)\n",
      "tensor(1.2172)\n",
      "tensor(1.1028)\n",
      "tensor(0.7874)\n",
      "tensor(1.0450)\n",
      "tensor(0.8673)\n",
      "tensor(1.4180)\n",
      "tensor(1.0158)\n",
      "tensor(0.9093)\n",
      "tensor(0.9683)\n",
      "tensor(1.4709)\n",
      "tensor(1.4060)\n",
      "tensor(1.3493)\n",
      "tensor(1.4085)\n",
      "tensor(1.3358)\n",
      "tensor(1.2971)\n",
      "tensor(1.3479)\n",
      "tensor(1.3091)\n",
      "tensor(1.2500)\n",
      "tensor(0.9109)\n",
      "tensor(1.4503)\n",
      "tensor(0.9287)\n",
      "tensor(0.7563)\n",
      "tensor(1.1702)\n",
      "tensor(1.6499)\n",
      "tensor(0.9557)\n",
      "tensor(1.2328)\n",
      "tensor(1.5239)\n",
      "tensor(1.1819)\n",
      "tensor(1.2714)\n",
      "tensor(0.9445)\n",
      "tensor(1.2934)\n",
      "tensor(1.1329)\n",
      "tensor(0.9186)\n",
      "tensor(1.4150)\n",
      "tensor(1.0881)\n",
      "tensor(0.9027)\n",
      "tensor(1.0657)\n",
      "tensor(1.0813)\n",
      "tensor(1.0913)\n",
      "tensor(1.1071)\n",
      "tensor(1.2630)\n",
      "tensor(1.1501)\n",
      "tensor(1.1437)\n",
      "tensor(1.0385)\n",
      "tensor(0.8923)\n",
      "tensor(1.3417)\n",
      "tensor(1.3359)\n",
      "tensor(1.2260)\n",
      "tensor(1.2586)\n",
      "tensor(1.2572)\n",
      "tensor(1.2851)\n",
      "tensor(1.2371)\n",
      "tensor(0.6376)\n",
      "tensor(0.8428)\n",
      "tensor(1.1697)\n",
      "tensor(0.8445)\n",
      "tensor(0.8789)\n",
      "tensor(0.6156)\n",
      "tensor(1.2946)\n",
      "tensor(1.5566)\n",
      "tensor(1.3936)\n",
      "tensor(1.7024)\n",
      "tensor(1.1432)\n",
      "tensor(1.0480)\n",
      "tensor(1.4907)\n",
      "tensor(1.4394)\n",
      "tensor(1.3937)\n",
      "tensor(1.3869)\n",
      "tensor(1.3443)\n",
      "tensor(1.3061)\n",
      "tensor(0.9848)\n",
      "tensor(1.3665)\n",
      "tensor(0.8909)\n",
      "tensor(1.4576)\n",
      "tensor(1.0299)\n",
      "tensor(0.7580)\n",
      "tensor(1.1389)\n",
      "tensor(1.4715)\n",
      "tensor(1.4153)\n",
      "tensor(1.2760)\n",
      "tensor(1.1730)\n",
      "tensor(1.0915)\n",
      "tensor(0.6833)\n",
      "tensor(1.4201)\n",
      "tensor(1.1287)\n",
      "tensor(1.4033)\n",
      "tensor(0.9762)\n",
      "tensor(0.8606)\n",
      "tensor(1.5226)\n",
      "tensor(1.0955)\n",
      "tensor(1.4329)\n",
      "tensor(1.1556)\n",
      "tensor(1.3154)\n",
      "tensor(1.0472)\n",
      "tensor(1.2895)\n",
      "tensor(1.1714)\n",
      "tensor(1.2131)\n",
      "tensor(0.9226)\n",
      "tensor(0.9931)\n",
      "tensor(0.9044)\n",
      "tensor(0.7204)\n",
      "tensor(1.3875)\n",
      "tensor(0.9556)\n",
      "tensor(1.3448)\n",
      "tensor(1.0874)\n",
      "tensor(0.8341)\n",
      "tensor(0.7753)\n",
      "tensor(1.3642)\n",
      "tensor(0.7919)\n",
      "tensor(1.4057)\n",
      "tensor(1.0792)\n",
      "tensor(0.7763)\n",
      "tensor(0.7282)\n",
      "tensor(1.3575)\n",
      "tensor(0.8399)\n",
      "tensor(1.3811)\n",
      "tensor(1.0404)\n",
      "tensor(0.7568)\n",
      "tensor(1.4026)\n",
      "tensor(1.2382)\n",
      "tensor(1.3327)\n",
      "tensor(1.3862)\n",
      "tensor(1.2986)\n",
      "tensor(0.9597)\n",
      "tensor(1.5359)\n",
      "tensor(1.3806)\n",
      "tensor(1.3962)\n",
      "tensor(1.3457)\n",
      "tensor(1.4311)\n",
      "tensor(1.2950)\n",
      "tensor(1.0710)\n",
      "tensor(1.6050)\n",
      "tensor(1.2009)\n",
      "tensor(1.4115)\n",
      "tensor(0.8511)\n",
      "tensor(0.9809)\n",
      "tensor(1.2848)\n",
      "tensor(1.4131)\n",
      "tensor(1.1066)\n",
      "tensor(1.3663)\n",
      "tensor(1.1209)\n",
      "tensor(0.8592)\n",
      "tensor(1.4457)\n",
      "tensor(1.3702)\n",
      "tensor(1.4601)\n",
      "tensor(0.7680)\n",
      "tensor(0.9412)\n",
      "tensor(1.2738)\n",
      "tensor(1.0721)\n",
      "tensor(1.4146)\n",
      "tensor(1.1955)\n",
      "tensor(1.3357)\n",
      "tensor(0.8832)\n",
      "tensor(1.1295)\n",
      "tensor(1.3423)\n",
      "tensor(0.9603)\n",
      "tensor(1.3295)\n",
      "tensor(1.1776)\n",
      "tensor(1.1453)\n",
      "tensor(0.8757)\n",
      "tensor(1.4448)\n",
      "tensor(1.1099)\n",
      "tensor(1.3242)\n",
      "tensor(0.8373)\n",
      "tensor(1.2469)\n",
      "tensor(1.0054)\n",
      "tensor(1.0122)\n",
      "tensor(1.4104)\n",
      "tensor(1.0283)\n",
      "tensor(1.2822)\n",
      "tensor(0.8740)\n",
      "tensor(1.0842)\n",
      "tensor(1.0658)\n",
      "tensor(1.3993)\n",
      "tensor(0.8809)\n",
      "tensor(1.3430)\n",
      "tensor(0.9337)\n",
      "tensor(1.0257)\n",
      "tensor(1.0833)\n",
      "tensor(1.3882)\n",
      "tensor(0.9144)\n",
      "tensor(1.3306)\n",
      "tensor(0.9401)\n",
      "tensor(1.0127)\n",
      "tensor(1.0390)\n",
      "tensor(1.2543)\n",
      "tensor(1.1379)\n",
      "tensor(1.2570)\n",
      "tensor(0.9003)\n",
      "tensor(0.6637)\n",
      "tensor(1.6333)\n",
      "tensor(1.3019)\n",
      "tensor(1.5514)\n",
      "tensor(1.4409)\n",
      "tensor(1.4691)\n",
      "tensor(1.1987)\n",
      "tensor(1.6369)\n",
      "tensor(0.8388)\n",
      "tensor(1.4648)\n",
      "tensor(1.0414)\n",
      "tensor(1.4292)\n",
      "tensor(1.3167)\n",
      "tensor(0.8225)\n",
      "tensor(1.3547)\n",
      "tensor(0.8202)\n",
      "tensor(1.3082)\n",
      "tensor(0.7739)\n",
      "tensor(0.8109)\n",
      "tensor(1.3718)\n",
      "tensor(1.2006)\n",
      "tensor(1.4773)\n",
      "tensor(0.9577)\n",
      "tensor(1.0341)\n",
      "tensor(1.0966)\n",
      "tensor(1.1310)\n",
      "tensor(1.2749)\n",
      "tensor(1.3145)\n",
      "tensor(1.2254)\n",
      "tensor(0.7673)\n",
      "tensor(0.9669)\n",
      "tensor(1.4444)\n",
      "tensor(0.6962)\n",
      "tensor(1.3383)\n",
      "tensor(1.3148)\n",
      "tensor(1.3004)\n",
      "tensor(0.7309)\n",
      "tensor(1.1932)\n",
      "tensor(1.3581)\n",
      "tensor(1.1018)\n",
      "tensor(0.8339)\n",
      "tensor(0.9716)\n",
      "tensor(0.8545)\n",
      "tensor(0.8036)\n",
      "tensor(1.2667)\n",
      "tensor(1.1648)\n",
      "tensor(1.3151)\n",
      "tensor(0.8937)\n",
      "tensor(0.9102)\n",
      "tensor(0.6076)\n",
      "tensor(1.2720)\n",
      "tensor(1.0354)\n",
      "tensor(1.3375)\n",
      "tensor(0.8820)\n",
      "tensor(0.8603)\n",
      "tensor(0.7029)\n",
      "tensor(1.2663)\n",
      "tensor(1.0782)\n",
      "tensor(1.3117)\n",
      "tensor(0.8325)\n",
      "tensor(0.8541)\n",
      "tensor(1.0294)\n",
      "tensor(1.3034)\n",
      "tensor(1.0639)\n",
      "tensor(1.2592)\n",
      "tensor(0.9473)\n",
      "tensor(1.1396)\n",
      "tensor(1.7982)\n",
      "tensor(1.3736)\n",
      "tensor(1.5874)\n",
      "tensor(0.7896)\n",
      "tensor(1.6235)\n",
      "tensor(1.2204)\n",
      "tensor(1.4059)\n",
      "tensor(1.4963)\n",
      "tensor(1.2769)\n",
      "tensor(0.9516)\n",
      "tensor(1.0435)\n",
      "tensor(0.9281)\n",
      "tensor(1.2275)\n",
      "tensor(1.5208)\n",
      "tensor(1.1036)\n",
      "tensor(1.0238)\n",
      "tensor(1.1092)\n",
      "tensor(1.3180)\n",
      "tensor(1.7100)\n",
      "tensor(0.9512)\n",
      "tensor(1.6235)\n",
      "tensor(1.2330)\n",
      "tensor(1.3539)\n",
      "tensor(1.2632)\n",
      "tensor(1.6124)\n",
      "tensor(1.1618)\n",
      "tensor(1.5527)\n",
      "tensor(1.2384)\n",
      "tensor(1.3506)\n",
      "tensor(1.3079)\n",
      "tensor(1.1703)\n",
      "tensor(1.1996)\n",
      "tensor(1.2381)\n",
      "tensor(1.2769)\n",
      "tensor(0.9792)\n",
      "tensor(1.1306)\n",
      "tensor(1.5132)\n",
      "tensor(1.3396)\n",
      "tensor(1.3920)\n",
      "tensor(1.3534)\n",
      "tensor(1.3705)\n",
      "tensor(1.1631)\n",
      "tensor(1.3343)\n",
      "tensor(1.2958)\n",
      "tensor(1.4571)\n",
      "tensor(1.4090)\n",
      "tensor(0.9472)\n",
      "tensor(1.3186)\n",
      "tensor(1.1270)\n",
      "tensor(1.3616)\n",
      "tensor(1.3400)\n",
      "tensor(1.3286)\n",
      "tensor(0.7165)\n",
      "tensor(1.3114)\n",
      "tensor(1.2323)\n",
      "tensor(1.3336)\n",
      "tensor(1.3778)\n",
      "tensor(1.3236)\n",
      "tensor(0.8597)\n",
      "tensor(1.3063)\n",
      "tensor(0.6296)\n",
      "tensor(1.3628)\n",
      "tensor(0.8854)\n",
      "tensor(1.0778)\n",
      "tensor(0.7786)\n",
      "tensor(0.8437)\n",
      "tensor(1.5059)\n",
      "tensor(1.4545)\n",
      "tensor(1.4088)\n",
      "tensor(1.4020)\n",
      "tensor(1.3595)\n",
      "tensor(1.3212)\n",
      "tensor(1.1997)\n",
      "tensor(1.6742)\n",
      "tensor(1.2895)\n",
      "tensor(1.4538)\n",
      "tensor(1.0027)\n",
      "tensor(1.0076)\n",
      "tensor(1.3208)\n",
      "tensor(1.4330)\n",
      "tensor(1.1430)\n",
      "tensor(1.4261)\n",
      "tensor(1.1452)\n",
      "tensor(0.8305)\n",
      "tensor(1.4152)\n",
      "tensor(1.4445)\n",
      "tensor(1.4759)\n",
      "tensor(0.9495)\n",
      "tensor(0.8314)\n",
      "tensor(1.2976)\n",
      "tensor(1.1545)\n",
      "tensor(1.4541)\n",
      "tensor(1.1661)\n",
      "tensor(1.4225)\n",
      "tensor(0.9607)\n",
      "tensor(1.1139)\n",
      "tensor(1.3949)\n",
      "tensor(1.0306)\n",
      "tensor(1.3816)\n",
      "tensor(1.1768)\n",
      "tensor(1.1908)\n",
      "tensor(0.8945)\n",
      "tensor(1.4770)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/miniconda3/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:57: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1047)\n",
      "tensor(1.3613)\n",
      "tensor(0.9372)\n",
      "tensor(1.2644)\n",
      "tensor(1.0420)\n",
      "tensor(1.0740)\n",
      "tensor(1.4485)\n",
      "tensor(0.9944)\n",
      "tensor(1.3041)\n",
      "tensor(0.9246)\n",
      "tensor(1.0769)\n",
      "tensor(1.1164)\n",
      "tensor(1.4328)\n",
      "tensor(0.9258)\n",
      "tensor(1.3795)\n",
      "tensor(0.9733)\n",
      "tensor(1.0160)\n",
      "tensor(1.1366)\n",
      "tensor(1.4207)\n",
      "tensor(0.9613)\n",
      "tensor(1.3664)\n",
      "tensor(0.9813)\n",
      "tensor(0.9991)\n",
      "tensor(1.1196)\n",
      "tensor(1.2857)\n",
      "tensor(1.2065)\n",
      "tensor(1.2961)\n",
      "tensor(0.9809)\n",
      "tensor(0.6728)\n",
      "tensor(1.2439)\n",
      "tensor(1.3799)\n",
      "tensor(1.4236)\n",
      "tensor(1.3274)\n",
      "tensor(1.4374)\n",
      "tensor(1.5775)\n",
      "tensor(1.2145)\n",
      "tensor(1.6065)\n",
      "tensor(1.4305)\n",
      "tensor(0.8086)\n",
      "tensor(1.2899)\n",
      "tensor(1.4785)\n",
      "tensor(1.2124)\n",
      "tensor(0.9300)\n",
      "tensor(1.1427)\n",
      "tensor(1.4315)\n",
      "tensor(1.2030)\n",
      "tensor(0.8984)\n",
      "tensor(1.1826)\n",
      "tensor(1.1341)\n",
      "tensor(1.2558)\n",
      "tensor(1.5875)\n",
      "tensor(1.2252)\n",
      "tensor(1.3697)\n",
      "tensor(1.1686)\n",
      "tensor(0.8770)\n",
      "tensor(1.2050)\n",
      "tensor(1.4396)\n",
      "tensor(1.2358)\n",
      "tensor(1.3065)\n",
      "tensor(1.1677)\n",
      "tensor(1.3175)\n",
      "tensor(1.3027)\n",
      "tensor(0.9699)\n",
      "tensor(1.1908)\n",
      "tensor(1.0606)\n",
      "tensor(1.1676)\n",
      "tensor(0.8111)\n",
      "tensor(1.2335)\n",
      "tensor(1.3170)\n",
      "tensor(1.2286)\n",
      "tensor(1.1521)\n",
      "tensor(1.1642)\n",
      "tensor(0.9663)\n",
      "tensor(1.0956)\n",
      "tensor(1.1726)\n",
      "tensor(1.1411)\n",
      "tensor(1.1132)\n",
      "tensor(1.1631)\n",
      "tensor(0.9860)\n",
      "tensor(1.0624)\n",
      "tensor(1.2280)\n",
      "tensor(1.1237)\n",
      "tensor(0.9975)\n",
      "tensor(1.1523)\n",
      "tensor(0.9301)\n",
      "tensor(1.0860)\n",
      "tensor(1.2336)\n",
      "tensor(1.1369)\n",
      "tensor(1.0620)\n",
      "tensor(1.1485)\n",
      "tensor(1.3258)\n",
      "tensor(1.3091)\n",
      "tensor(0.7709)\n",
      "tensor(1.0934)\n",
      "tensor(1.0378)\n",
      "tensor(1.4840)\n",
      "tensor(1.4919)\n",
      "tensor(1.1325)\n",
      "tensor(1.1484)\n",
      "tensor(1.3923)\n",
      "tensor(1.5372)\n",
      "tensor(1.4928)\n",
      "tensor(1.5409)\n",
      "tensor(1.1030)\n",
      "tensor(0.9319)\n",
      "tensor(1.3587)\n",
      "tensor(1.4113)\n",
      "tensor(1.1926)\n",
      "tensor(0.7315)\n",
      "tensor(1.1010)\n",
      "tensor(1.3222)\n",
      "tensor(1.1153)\n",
      "tensor(1.1422)\n",
      "tensor(1.3121)\n",
      "tensor(1.4122)\n",
      "tensor(1.0711)\n",
      "tensor(1.4269)\n",
      "tensor(1.1235)\n",
      "tensor(1.4451)\n",
      "tensor(1.2597)\n",
      "tensor(1.2180)\n",
      "tensor(1.0572)\n",
      "tensor(1.3296)\n",
      "tensor(1.1447)\n",
      "tensor(1.3888)\n",
      "tensor(1.3619)\n",
      "tensor(1.3006)\n",
      "tensor(1.0563)\n",
      "tensor(1.1674)\n",
      "tensor(1.2780)\n",
      "tensor(1.0392)\n",
      "tensor(1.2895)\n",
      "tensor(0.8800)\n",
      "tensor(1.0562)\n",
      "tensor(1.3463)\n",
      "tensor(1.2376)\n",
      "tensor(1.1955)\n",
      "tensor(1.1455)\n",
      "tensor(1.0589)\n",
      "tensor(1.0528)\n",
      "tensor(1.2803)\n",
      "tensor(1.0167)\n",
      "tensor(1.2989)\n",
      "tensor(1.1108)\n",
      "tensor(0.9297)\n",
      "tensor(1.0516)\n",
      "tensor(1.1048)\n",
      "tensor(0.9949)\n",
      "tensor(1.2240)\n",
      "tensor(1.1360)\n",
      "tensor(0.9745)\n",
      "tensor(1.0409)\n",
      "tensor(1.1459)\n",
      "tensor(1.0245)\n",
      "tensor(1.2496)\n",
      "tensor(1.3696)\n",
      "tensor(1.2962)\n",
      "tensor(1.0370)\n",
      "tensor(0.9075)\n",
      "tensor(1.2165)\n",
      "tensor(1.0500)\n",
      "tensor(1.2336)\n",
      "tensor(1.6358)\n",
      "tensor(1.1813)\n",
      "tensor(1.2530)\n",
      "tensor(1.0836)\n",
      "tensor(1.2032)\n",
      "tensor(0.6754)\n",
      "tensor(1.6149)\n",
      "tensor(1.0608)\n",
      "tensor(1.1673)\n",
      "tensor(0.9637)\n",
      "tensor(0.8908)\n",
      "tensor(1.4855)\n",
      "tensor(1.3779)\n",
      "tensor(1.3100)\n",
      "tensor(1.2660)\n",
      "tensor(1.3313)\n",
      "tensor(0.9291)\n",
      "tensor(1.6284)\n",
      "tensor(1.2482)\n",
      "tensor(1.4039)\n",
      "tensor(1.3027)\n",
      "tensor(1.4467)\n",
      "tensor(1.1720)\n",
      "tensor(1.3634)\n",
      "tensor(1.3121)\n",
      "tensor(1.2664)\n",
      "tensor(1.2596)\n",
      "tensor(1.2170)\n",
      "tensor(1.1788)\n",
      "tensor(1.0926)\n",
      "tensor(1.0073)\n",
      "tensor(1.2180)\n",
      "tensor(1.0084)\n",
      "tensor(0.8189)\n",
      "tensor(1.0277)\n",
      "tensor(1.4012)\n",
      "tensor(0.8522)\n",
      "tensor(1.3557)\n",
      "tensor(1.0979)\n",
      "tensor(1.1574)\n",
      "tensor(1.2305)\n",
      "tensor(1.3069)\n",
      "tensor(1.3743)\n",
      "tensor(1.1281)\n",
      "tensor(0.9552)\n",
      "tensor(1.2243)\n",
      "tensor(1.1658)\n",
      "tensor(1.3425)\n",
      "tensor(1.3805)\n",
      "tensor(1.1371)\n",
      "tensor(1.1385)\n",
      "tensor(1.2476)\n",
      "tensor(1.1140)\n",
      "tensor(1.3555)\n",
      "tensor(1.3530)\n",
      "tensor(1.1686)\n",
      "tensor(1.1539)\n",
      "tensor(1.2532)\n",
      "tensor(1.0891)\n",
      "tensor(0.7748)\n",
      "tensor(1.2566)\n",
      "tensor(0.9955)\n",
      "tensor(1.0592)\n",
      "tensor(1.0038)\n",
      "tensor(0.6639)\n",
      "tensor(1.2308)\n",
      "tensor(1.4772)\n",
      "tensor(1.3202)\n",
      "tensor(1.6123)\n",
      "tensor(1.0808)\n",
      "tensor(0.9892)\n",
      "tensor(1.4154)\n",
      "tensor(1.3641)\n",
      "tensor(1.3184)\n",
      "tensor(1.3116)\n",
      "tensor(1.2690)\n",
      "tensor(1.2308)\n",
      "tensor(0.9350)\n",
      "tensor(1.2949)\n",
      "tensor(0.8427)\n",
      "tensor(1.3783)\n",
      "tensor(0.9721)\n",
      "tensor(0.7143)\n",
      "tensor(1.0799)\n",
      "tensor(1.3924)\n",
      "tensor(1.3366)\n",
      "tensor(1.2047)\n",
      "tensor(1.1053)\n",
      "tensor(1.0267)\n",
      "tensor(0.6475)\n",
      "tensor(1.3429)\n",
      "tensor(1.0652)\n",
      "tensor(1.3240)\n",
      "tensor(0.9192)\n",
      "tensor(0.8088)\n",
      "tensor(1.4427)\n",
      "tensor(1.0359)\n",
      "tensor(1.3522)\n",
      "tensor(1.0902)\n",
      "tensor(1.2385)\n",
      "tensor(0.9841)\n",
      "tensor(1.2218)\n",
      "tensor(1.1076)\n",
      "tensor(1.1448)\n",
      "tensor(0.8704)\n",
      "tensor(0.9350)\n",
      "tensor(0.8499)\n",
      "tensor(0.6825)\n",
      "tensor(1.3117)\n",
      "tensor(0.9016)\n",
      "tensor(1.2684)\n",
      "tensor(1.0236)\n",
      "tensor(0.7836)\n",
      "tensor(0.7345)\n",
      "tensor(1.2896)\n",
      "tensor(0.7472)\n",
      "tensor(1.3258)\n",
      "tensor(1.0158)\n",
      "tensor(0.7293)\n",
      "tensor(0.6894)\n",
      "tensor(1.2825)\n",
      "tensor(0.7919)\n",
      "tensor(1.3018)\n",
      "tensor(0.9787)\n",
      "tensor(0.7105)\n",
      "tensor(1.3277)\n",
      "tensor(1.1697)\n",
      "tensor(1.2563)\n",
      "tensor(1.3063)\n",
      "tensor(1.2213)\n",
      "tensor(0.9008)\n",
      "tensor(1.3269)\n",
      "tensor(1.5859)\n",
      "tensor(1.3959)\n",
      "tensor(0.9155)\n",
      "tensor(0.7779)\n",
      "tensor(1.4192)\n",
      "tensor(1.0081)\n",
      "tensor(1.6425)\n",
      "tensor(1.2586)\n",
      "tensor(1.0878)\n",
      "tensor(1.0564)\n",
      "tensor(1.1434)\n",
      "tensor(1.4405)\n",
      "tensor(1.2616)\n",
      "tensor(1.2809)\n",
      "tensor(1.3550)\n",
      "tensor(1.2269)\n",
      "tensor(0.7398)\n",
      "tensor(1.3761)\n",
      "tensor(1.3248)\n",
      "tensor(1.2790)\n",
      "tensor(1.2723)\n",
      "tensor(1.2297)\n",
      "tensor(1.1915)\n",
      "tensor(1.4571)\n",
      "tensor(1.2039)\n",
      "tensor(1.3005)\n",
      "tensor(1.4886)\n",
      "tensor(1.2551)\n",
      "tensor(1.2023)\n",
      "tensor(1.1671)\n",
      "tensor(1.1014)\n",
      "tensor(1.2707)\n",
      "tensor(0.9047)\n",
      "tensor(0.9332)\n",
      "tensor(0.9640)\n",
      "tensor(1.3722)\n",
      "tensor(0.7475)\n",
      "tensor(1.3366)\n",
      "tensor(1.1931)\n",
      "tensor(1.0695)\n",
      "tensor(1.1584)\n",
      "tensor(1.3972)\n",
      "tensor(1.3028)\n",
      "tensor(1.1869)\n",
      "tensor(0.9855)\n",
      "tensor(1.2477)\n",
      "tensor(0.9341)\n",
      "tensor(1.3771)\n",
      "tensor(1.2988)\n",
      "tensor(1.1657)\n",
      "tensor(1.1618)\n",
      "tensor(1.2171)\n",
      "tensor(0.8886)\n",
      "tensor(1.3885)\n",
      "tensor(1.2626)\n",
      "tensor(1.1928)\n",
      "tensor(1.1699)\n",
      "tensor(1.2148)\n",
      "tensor(0.8917)\n",
      "tensor(0.9090)\n",
      "tensor(1.2293)\n",
      "tensor(1.1273)\n",
      "tensor(1.0240)\n",
      "tensor(1.0422)\n",
      "tensor(0.6121)\n",
      "tensor(1.7239)\n",
      "tensor(0.7751)\n",
      "tensor(1.6225)\n",
      "tensor(1.1140)\n",
      "tensor(1.4919)\n",
      "tensor(1.1754)\n",
      "tensor(1.6257)\n",
      "tensor(0.9972)\n",
      "tensor(1.5000)\n",
      "tensor(0.8414)\n",
      "tensor(1.3550)\n",
      "tensor(1.1464)\n",
      "tensor(1.1109)\n",
      "tensor(1.0308)\n",
      "tensor(1.2789)\n",
      "tensor(1.0803)\n",
      "tensor(0.8226)\n",
      "tensor(1.0311)\n",
      "tensor(1.5138)\n",
      "tensor(0.8217)\n",
      "tensor(1.5252)\n",
      "tensor(1.2089)\n",
      "tensor(1.1789)\n",
      "tensor(1.0776)\n",
      "tensor(1.4691)\n",
      "tensor(0.6091)\n",
      "tensor(1.4823)\n",
      "tensor(1.2086)\n",
      "tensor(1.1852)\n",
      "tensor(1.0587)\n",
      "tensor(1.2982)\n",
      "tensor(1.5165)\n",
      "tensor(1.1910)\n",
      "tensor(0.9635)\n",
      "tensor(1.0978)\n",
      "tensor(1.0134)\n",
      "tensor(1.3953)\n",
      "tensor(1.0281)\n",
      "tensor(1.3467)\n",
      "tensor(1.4462)\n",
      "tensor(1.2304)\n",
      "tensor(0.9868)\n",
      "tensor(1.2858)\n",
      "tensor(0.7243)\n",
      "tensor(1.4005)\n",
      "tensor(1.2947)\n",
      "tensor(0.9578)\n",
      "tensor(1.0302)\n",
      "tensor(1.1791)\n",
      "tensor(0.8148)\n",
      "tensor(1.3341)\n",
      "tensor(1.2581)\n",
      "tensor(0.8522)\n",
      "tensor(1.0146)\n",
      "tensor(1.2418)\n",
      "tensor(0.7751)\n",
      "tensor(1.3573)\n",
      "tensor(1.2422)\n",
      "tensor(0.9308)\n",
      "tensor(1.0105)\n",
      "tensor(1.2128)\n",
      "tensor(1.3136)\n",
      "tensor(1.1615)\n",
      "tensor(0.6428)\n",
      "tensor(0.9035)\n",
      "tensor(1.0301)\n",
      "tensor(0.8480)\n",
      "tensor(1.6423)\n",
      "tensor(1.2763)\n",
      "tensor(0.8974)\n",
      "tensor(0.9986)\n",
      "tensor(1.4190)\n",
      "tensor(0.9107)\n",
      "tensor(1.6121)\n",
      "tensor(1.1252)\n",
      "tensor(0.9263)\n",
      "tensor(1.2475)\n",
      "tensor(1.1498)\n",
      "tensor(1.4358)\n",
      "tensor(1.1212)\n",
      "tensor(1.3740)\n",
      "tensor(1.2189)\n",
      "tensor(1.1351)\n",
      "tensor(1.0140)\n",
      "tensor(1.1386)\n",
      "tensor(1.4880)\n",
      "tensor(1.1935)\n",
      "tensor(0.9549)\n",
      "tensor(1.3102)\n",
      "tensor(1.2728)\n",
      "tensor(1.2193)\n",
      "tensor(1.3408)\n",
      "tensor(1.3168)\n",
      "tensor(1.2917)\n",
      "tensor(0.7149)\n",
      "tensor(1.3449)\n",
      "tensor(0.9119)\n",
      "tensor(1.1247)\n",
      "tensor(1.1641)\n",
      "tensor(1.1159)\n",
      "tensor(1.0725)\n",
      "tensor(1.0236)\n",
      "tensor(1.2002)\n",
      "tensor(0.8583)\n",
      "tensor(1.2984)\n",
      "tensor(1.2902)\n",
      "tensor(0.7982)\n",
      "tensor(1.2293)\n",
      "tensor(1.3966)\n",
      "tensor(1.1189)\n",
      "tensor(1.3319)\n",
      "tensor(1.2335)\n",
      "tensor(1.0851)\n",
      "tensor(1.2004)\n",
      "tensor(1.4165)\n",
      "tensor(1.1582)\n",
      "tensor(1.3177)\n",
      "tensor(0.8409)\n",
      "tensor(1.1390)\n",
      "tensor(1.0876)\n",
      "tensor(1.3820)\n",
      "tensor(1.0887)\n",
      "tensor(1.3171)\n",
      "tensor(0.9241)\n",
      "tensor(1.0725)\n",
      "tensor(1.1569)\n",
      "tensor(1.0602)\n",
      "tensor(1.2287)\n",
      "tensor(1.0040)\n",
      "tensor(0.9428)\n",
      "tensor(1.1651)\n",
      "tensor(0.6561)\n",
      "tensor(1.0367)\n",
      "tensor(1.6137)\n",
      "tensor(1.2761)\n",
      "tensor(0.8575)\n",
      "tensor(0.6298)\n",
      "tensor(1.3402)\n",
      "tensor(0.7487)\n",
      "tensor(1.6000)\n",
      "tensor(1.1320)\n",
      "tensor(0.9696)\n",
      "tensor(1.0912)\n",
      "tensor(1.0485)\n",
      "tensor(1.4588)\n",
      "tensor(1.2450)\n",
      "tensor(1.3152)\n",
      "tensor(1.2614)\n",
      "tensor(1.2469)\n",
      "tensor(0.8221)\n",
      "tensor(1.4404)\n",
      "tensor(1.3690)\n",
      "tensor(1.2003)\n",
      "tensor(1.2196)\n",
      "tensor(1.4210)\n",
      "tensor(1.1099)\n",
      "tensor(1.3750)\n",
      "tensor(1.2416)\n",
      "tensor(1.2961)\n",
      "tensor(1.3867)\n",
      "tensor(1.1186)\n",
      "tensor(1.2456)\n",
      "tensor(1.0429)\n",
      "tensor(1.0595)\n",
      "tensor(1.1967)\n",
      "tensor(0.9960)\n",
      "tensor(0.9262)\n",
      "tensor(0.9881)\n",
      "tensor(1.3165)\n",
      "tensor(0.7676)\n",
      "tensor(1.3136)\n",
      "tensor(1.2015)\n",
      "tensor(1.0075)\n",
      "tensor(1.1885)\n",
      "tensor(1.4110)\n",
      "tensor(1.2719)\n",
      "tensor(1.2298)\n",
      "tensor(0.8287)\n",
      "tensor(1.2605)\n",
      "tensor(1.0010)\n",
      "tensor(1.4025)\n",
      "tensor(1.2776)\n",
      "tensor(1.2108)\n",
      "tensor(0.9919)\n",
      "tensor(1.2446)\n",
      "tensor(0.9306)\n",
      "tensor(1.4030)\n",
      "tensor(1.2341)\n",
      "tensor(1.2317)\n",
      "tensor(1.0135)\n",
      "tensor(1.2300)\n",
      "tensor(0.9686)\n",
      "tensor(0.8703)\n",
      "tensor(1.2150)\n",
      "tensor(1.0224)\n",
      "tensor(0.9295)\n",
      "tensor(1.0624)\n",
      "tensor(0.6059)\n",
      "tensor(1.3279)\n",
      "tensor(1.1137)\n",
      "tensor(1.3713)\n",
      "tensor(1.5009)\n",
      "tensor(1.1935)\n",
      "tensor(0.5845)\n",
      "tensor(1.5445)\n",
      "tensor(1.0842)\n",
      "tensor(1.4131)\n",
      "tensor(1.4578)\n",
      "tensor(1.4431)\n",
      "tensor(0.8301)\n",
      "tensor(0.8941)\n",
      "tensor(1.0821)\n",
      "tensor(0.6707)\n",
      "tensor(1.2942)\n",
      "tensor(1.0394)\n",
      "tensor(1.0014)\n",
      "tensor(1.0891)\n",
      "tensor(1.0523)\n",
      "tensor(1.2917)\n",
      "tensor(1.1527)\n",
      "tensor(1.1394)\n",
      "tensor(0.6189)\n",
      "tensor(0.8418)\n",
      "tensor(1.0383)\n",
      "tensor(1.1127)\n",
      "tensor(1.2211)\n",
      "tensor(1.0207)\n",
      "tensor(0.6556)\n",
      "tensor(1.2646)\n",
      "tensor(1.0374)\n",
      "tensor(1.1881)\n",
      "tensor(0.9259)\n",
      "tensor(1.0435)\n",
      "tensor(1.2449)\n",
      "tensor(0.7785)\n",
      "tensor(1.0374)\n",
      "tensor(0.8039)\n",
      "tensor(0.9091)\n",
      "tensor(0.6255)\n",
      "tensor(1.0390)\n",
      "tensor(0.9273)\n",
      "tensor(1.0339)\n",
      "tensor(0.9671)\n",
      "tensor(1.2260)\n",
      "tensor(1.0832)\n",
      "tensor(0.7593)\n",
      "tensor(0.9462)\n",
      "tensor(1.0328)\n",
      "tensor(0.8489)\n",
      "tensor(1.2605)\n",
      "tensor(1.0744)\n",
      "tensor(0.8241)\n",
      "tensor(0.8922)\n",
      "tensor(1.0220)\n",
      "tensor(0.8890)\n",
      "tensor(1.2338)\n",
      "tensor(1.0437)\n",
      "tensor(0.7954)\n",
      "tensor(1.2715)\n",
      "tensor(1.0182)\n",
      "tensor(1.1821)\n",
      "tensor(1.1571)\n",
      "tensor(1.0883)\n",
      "tensor(1.1282)\n",
      "tensor(1.8448)\n",
      "tensor(1.8639)\n",
      "tensor(2.0880)\n",
      "tensor(1.5679)\n",
      "tensor(1.7927)\n",
      "tensor(2.0449)\n",
      "tensor(2.0263)\n",
      "tensor(1.8554)\n",
      "tensor(1.9130)\n",
      "tensor(1.8780)\n",
      "tensor(1.9314)\n",
      "tensor(1.3619)\n",
      "tensor(1.7214)\n",
      "tensor(1.6273)\n",
      "tensor(1.2645)\n",
      "tensor(1.2994)\n",
      "tensor(2.1018)\n",
      "tensor(1.3057)\n",
      "tensor(1.4466)\n",
      "tensor(1.5070)\n",
      "tensor(1.4508)\n",
      "tensor(2.0792)\n",
      "tensor(1.4539)\n",
      "tensor(1.5006)\n",
      "tensor(1.6106)\n",
      "tensor(1.1636)\n",
      "tensor(1.3241)\n",
      "tensor(1.6873)\n",
      "tensor(1.6320)\n",
      "tensor(1.3269)\n",
      "tensor(1.3865)\n",
      "tensor(1.3690)\n",
      "tensor(1.5656)\n",
      "tensor(1.4693)\n",
      "tensor(1.3433)\n",
      "tensor(2.4882)\n",
      "tensor(1.5005)\n",
      "tensor(1.9018)\n",
      "tensor(1.5877)\n",
      "tensor(2.1537)\n",
      "tensor(1.9543)\n",
      "tensor(2.1752)\n",
      "tensor(2.1438)\n",
      "tensor(1.5820)\n",
      "tensor(2.2171)\n",
      "tensor(2.1582)\n",
      "tensor(2.1541)\n",
      "tensor(2.0944)\n",
      "tensor(2.0665)\n",
      "tensor(2.0646)\n",
      "tensor(2.0645)\n",
      "tensor(2.0576)\n",
      "tensor(2.0554)\n",
      "tensor(2.0338)\n",
      "tensor(2.0262)\n",
      "tensor(1.3724)\n",
      "tensor(1.3410)\n",
      "tensor(1.1310)\n",
      "tensor(1.0724)\n",
      "tensor(1.1706)\n",
      "tensor(1.6112)\n",
      "tensor(1.6138)\n",
      "tensor(1.0485)\n",
      "tensor(1.0124)\n",
      "tensor(1.0582)\n",
      "tensor(1.1679)\n",
      "tensor(1.9042)\n",
      "tensor(2.8012)\n",
      "tensor(1.8565)\n",
      "tensor(2.2669)\n",
      "tensor(1.8027)\n",
      "tensor(1.7671)\n",
      "tensor(1.9745)\n",
      "tensor(2.1101)\n",
      "tensor(2.0145)\n",
      "tensor(2.1793)\n",
      "tensor(2.3700)\n",
      "tensor(1.7455)\n",
      "tensor(1.9804)\n",
      "tensor(2.0535)\n",
      "tensor(1.9821)\n",
      "tensor(2.1549)\n",
      "tensor(1.8089)\n",
      "tensor(1.6337)\n",
      "tensor(2.0055)\n",
      "tensor(2.0676)\n",
      "tensor(2.4565)\n",
      "tensor(2.4262)\n",
      "tensor(1.7275)\n",
      "tensor(1.2388)\n",
      "tensor(1.7384)\n",
      "tensor(2.0254)\n",
      "tensor(1.5007)\n",
      "tensor(1.2902)\n",
      "tensor(1.6119)\n",
      "tensor(1.5735)\n",
      "tensor(2.3395)\n",
      "tensor(1.6924)\n",
      "tensor(1.4657)\n",
      "tensor(1.3472)\n",
      "tensor(1.4277)\n",
      "tensor(2.0664)\n",
      "tensor(1.5154)\n",
      "tensor(1.6300)\n",
      "tensor(1.2416)\n",
      "tensor(1.7304)\n",
      "tensor(2.1001)\n",
      "tensor(2.3793)\n",
      "tensor(1.3892)\n",
      "tensor(1.5706)\n",
      "tensor(1.4146)\n",
      "tensor(1.6247)\n",
      "tensor(1.1691)\n",
      "tensor(1.3831)\n",
      "tensor(1.2717)\n",
      "tensor(1.4989)\n",
      "tensor(2.2293)\n",
      "tensor(1.4054)\n",
      "tensor(1.1337)\n",
      "tensor(1.2608)\n",
      "tensor(1.1528)\n",
      "tensor(1.9687)\n",
      "tensor(1.2000)\n",
      "tensor(1.3649)\n",
      "tensor(1.2514)\n",
      "tensor(1.5126)\n",
      "tensor(1.2165)\n",
      "tensor(1.1617)\n",
      "tensor(1.2433)\n",
      "tensor(1.1005)\n",
      "tensor(1.1398)\n",
      "tensor(1.1640)\n",
      "tensor(1.2978)\n",
      "tensor(0.4943)\n",
      "tensor(0.8435)\n",
      "tensor(1.3698)\n",
      "tensor(0.9824)\n",
      "tensor(1.2107)\n",
      "tensor(1.6132)\n",
      "tensor(1.0700)\n",
      "tensor(1.1290)\n",
      "tensor(0.7801)\n",
      "tensor(1.5335)\n",
      "tensor(1.4864)\n",
      "tensor(1.0401)\n",
      "tensor(0.9245)\n",
      "tensor(1.2793)\n",
      "tensor(1.2469)\n",
      "tensor(0.7004)\n",
      "tensor(1.1746)\n",
      "tensor(1.1374)\n",
      "tensor(1.1267)\n",
      "tensor(1.3142)\n",
      "tensor(1.0096)\n",
      "tensor(1.3044)\n",
      "tensor(0.9755)\n",
      "tensor(0.9460)\n",
      "tensor(1.1018)\n",
      "tensor(1.4566)\n",
      "tensor(0.7428)\n",
      "tensor(1.0983)\n",
      "tensor(1.2625)\n",
      "tensor(0.7534)\n",
      "tensor(1.1690)\n",
      "tensor(1.0615)\n",
      "tensor(1.0901)\n",
      "tensor(1.4056)\n",
      "tensor(0.8418)\n",
      "tensor(0.8262)\n",
      "tensor(1.1257)\n",
      "tensor(1.0735)\n",
      "tensor(1.2227)\n",
      "tensor(1.2478)\n",
      "tensor(1.3934)\n",
      "tensor(1.1959)\n",
      "tensor(1.0122)\n",
      "tensor(0.9631)\n",
      "tensor(0.7257)\n",
      "tensor(1.0610)\n",
      "tensor(1.1649)\n",
      "tensor(1.2059)\n",
      "tensor(1.1952)\n",
      "tensor(1.3751)\n",
      "tensor(0.6412)\n",
      "tensor(1.2113)\n",
      "tensor(0.8283)\n",
      "tensor(0.9392)\n",
      "tensor(1.1944)\n",
      "tensor(1.0752)\n",
      "tensor(1.1636)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos,features,labels = list(zip(*test_data[501]['points']))\n",
    "\n",
    "classs = get_class(features,pos).argmax()\n",
    "classs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== max vals\n",
      "tensor([0.0000, 0.3024, 0.0000, 0.3143, 0.3022, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.3392, 0.4216, 0.3396, 0.4093, 0.4113, 0.3666, 0.4124, 0.4061, 0.5725,\n",
      "        0.4225, 0.3251, 0.3803, 0.6357, 0.4421, 0.3478, 0.3950, 0.4419, 0.3999,\n",
      "        0.4118, 0.5327, 0.4016, 0.0000, 0.3362, 0.3833, 0.3759, 0.4827, 0.3917,\n",
      "        0.3481, 0.3626, 0.0000, 0.3538, 0.3495, 0.3265, 0.0000, 0.5074, 0.3649,\n",
      "        0.3629, 0.0000, 0.3297, 0.3872, 0.4616])\n",
      "--------------------\n",
      "===== cons_score\n",
      "tensor([0.3143, 0.6165, 0.3143, 0.3024, 0.3022, 0.3392, 0.3024, 0.0000, 1.1705,\n",
      "        1.1602, 1.1904, 1.3910, 1.1143, 1.4795, 1.4492, 1.4177, 1.3696, 0.3626,\n",
      "        0.3362, 0.7106, 1.2282, 1.2124, 1.1131, 1.0543, 0.6893, 0.8703, 1.2352,\n",
      "        1.3562, 1.0388, 0.7278, 0.8372])\n",
      "--------------------\n",
      "===== cons_score_masked\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1705,\n",
      "        1.1602, 1.1904, 1.3910, 1.1143, 1.4795, 1.4492, 1.4177, 1.3696, 0.0000,\n",
      "        0.0000, 0.0000, 1.2282, 1.2124, 1.1131, 1.0543, 0.0000, 0.8703, 1.2352,\n",
      "        1.3562, 1.0388, 0.0000, 0.8372])\n",
      "--------------------\n",
      "===== clas_score\n",
      "tensor([0.0000, 1.2053, 1.4290, 0.5923, 0.7990])\n",
      "--------------------\n",
      "tensor(2) 1\n"
     ]
    }
   ],
   "source": [
    "ix = 205\n",
    "pos,features,labels = list(zip(*test_data[ix]['points']))\n",
    "print(get_class(features,pos,True).argmax(),test_data[ix]['class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f44a30e9990>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "plt.imshow(confusion_m, cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200.,   0.,   0.,   0.,   0.],\n",
       "       [  0., 117.,  73.,   4.,   6.],\n",
       "       [  0.,   7., 188.,   2.,   3.],\n",
       "       [  6.,  11.,  11., 116.,  56.],\n",
       "       [  0.,   1.,   7.,   3., 189.]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/miniconda3/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:57: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True\n",
      "1 True\n",
      "2 True\n",
      "3 True\n",
      "4 True\n",
      "5 True\n",
      "6 True\n",
      "7 True\n",
      "8 True\n",
      "9 True\n",
      "10 True\n",
      "11 True\n",
      "12 True\n",
      "13 True\n",
      "14 True\n",
      "15 True\n",
      "16 True\n",
      "17 True\n",
      "18 True\n",
      "19 True\n",
      "20 True\n",
      "21 True\n",
      "22 True\n",
      "23 True\n",
      "24 True\n",
      "25 True\n",
      "26 True\n",
      "27 True\n",
      "28 True\n",
      "29 True\n",
      "30 True\n",
      "31 True\n",
      "32 True\n",
      "33 True\n",
      "34 True\n",
      "35 True\n",
      "36 True\n",
      "37 True\n",
      "38 True\n",
      "39 True\n",
      "40 True\n",
      "41 True\n",
      "42 True\n",
      "43 True\n",
      "44 True\n",
      "45 True\n",
      "46 True\n",
      "47 True\n",
      "48 True\n",
      "49 True\n",
      "50 True\n",
      "51 True\n",
      "52 True\n",
      "53 True\n",
      "54 True\n",
      "55 True\n",
      "56 True\n",
      "57 True\n",
      "58 True\n",
      "59 True\n",
      "60 True\n",
      "61 True\n",
      "62 True\n",
      "63 True\n",
      "64 True\n",
      "65 True\n",
      "66 True\n",
      "67 True\n",
      "68 True\n",
      "69 True\n",
      "70 True\n",
      "71 True\n",
      "72 True\n",
      "73 True\n",
      "74 True\n",
      "75 True\n",
      "76 True\n",
      "77 True\n",
      "78 True\n",
      "79 True\n",
      "80 True\n",
      "81 True\n",
      "82 True\n",
      "83 True\n",
      "84 True\n",
      "85 True\n",
      "86 True\n",
      "87 True\n",
      "88 True\n",
      "89 True\n",
      "90 True\n",
      "91 True\n",
      "92 True\n",
      "93 True\n",
      "94 True\n",
      "95 True\n",
      "96 True\n",
      "97 True\n",
      "98 True\n",
      "99 True\n",
      "100 True\n",
      "101 True\n",
      "102 True\n",
      "103 True\n",
      "104 True\n",
      "105 True\n",
      "106 True\n",
      "107 True\n",
      "108 True\n",
      "109 True\n",
      "110 True\n",
      "111 True\n",
      "112 True\n",
      "113 True\n",
      "114 True\n",
      "115 True\n",
      "116 True\n",
      "117 True\n",
      "118 True\n",
      "119 True\n",
      "120 True\n",
      "121 True\n",
      "122 True\n",
      "123 True\n",
      "124 True\n",
      "125 True\n",
      "126 True\n",
      "127 True\n",
      "128 True\n",
      "129 True\n",
      "130 True\n",
      "131 True\n",
      "132 True\n",
      "133 True\n",
      "134 True\n",
      "135 True\n",
      "136 True\n",
      "137 True\n",
      "138 True\n",
      "139 True\n",
      "140 True\n",
      "141 True\n",
      "142 True\n",
      "143 True\n",
      "144 True\n",
      "145 True\n",
      "146 True\n",
      "147 True\n",
      "148 True\n",
      "149 True\n",
      "150 True\n",
      "151 True\n",
      "152 True\n",
      "153 True\n",
      "154 True\n",
      "155 True\n",
      "156 True\n",
      "157 True\n",
      "158 True\n",
      "159 True\n",
      "160 True\n",
      "161 True\n",
      "162 True\n",
      "163 True\n",
      "164 True\n",
      "165 True\n",
      "166 True\n",
      "167 True\n",
      "168 True\n",
      "169 True\n",
      "170 True\n",
      "171 True\n",
      "172 True\n",
      "173 True\n",
      "174 True\n",
      "175 True\n",
      "176 True\n",
      "177 True\n",
      "178 True\n",
      "179 True\n",
      "180 True\n",
      "181 True\n",
      "182 True\n",
      "183 True\n",
      "184 True\n",
      "185 True\n",
      "186 True\n",
      "187 True\n",
      "188 True\n",
      "189 True\n",
      "190 True\n",
      "191 True\n",
      "192 True\n",
      "193 True\n",
      "194 True\n",
      "195 True\n",
      "196 True\n",
      "197 True\n",
      "198 True\n",
      "199 True\n",
      "200 False\n",
      "201 True\n",
      "202 True\n",
      "203 False\n",
      "204 False\n",
      "205 False\n",
      "206 False\n",
      "207 False\n",
      "208 False\n",
      "209 False\n",
      "210 False\n",
      "211 False\n",
      "212 True\n",
      "213 True\n",
      "214 False\n",
      "215 False\n",
      "216 True\n",
      "217 True\n",
      "218 True\n",
      "219 True\n",
      "220 False\n",
      "221 False\n",
      "222 True\n",
      "223 True\n",
      "224 True\n",
      "225 False\n",
      "226 True\n",
      "227 True\n",
      "228 False\n",
      "229 False\n",
      "230 False\n",
      "231 False\n",
      "232 True\n",
      "233 False\n",
      "234 True\n",
      "235 True\n",
      "236 True\n",
      "237 False\n",
      "238 True\n",
      "239 True\n",
      "240 True\n",
      "241 True\n",
      "242 True\n",
      "243 True\n",
      "244 True\n",
      "245 True\n",
      "246 True\n",
      "247 True\n",
      "248 True\n",
      "249 False\n",
      "250 True\n",
      "251 True\n",
      "252 False\n",
      "253 False\n",
      "254 True\n",
      "255 True\n",
      "256 True\n",
      "257 True\n",
      "258 True\n",
      "259 False\n",
      "260 True\n",
      "261 False\n",
      "262 False\n",
      "263 True\n",
      "264 False\n",
      "265 True\n",
      "266 True\n",
      "267 True\n",
      "268 True\n",
      "269 True\n",
      "270 True\n",
      "271 True\n",
      "272 True\n",
      "273 True\n",
      "274 False\n",
      "275 False\n",
      "276 False\n",
      "277 True\n",
      "278 True\n",
      "279 False\n",
      "280 True\n",
      "281 False\n",
      "282 True\n",
      "283 False\n",
      "284 True\n",
      "285 True\n",
      "286 True\n",
      "287 True\n",
      "288 True\n",
      "289 True\n",
      "290 False\n",
      "291 True\n",
      "292 False\n",
      "293 True\n",
      "294 False\n",
      "295 True\n",
      "296 False\n",
      "297 True\n",
      "298 False\n",
      "299 False\n",
      "300 False\n",
      "301 True\n",
      "302 True\n",
      "303 True\n",
      "304 False\n",
      "305 False\n",
      "306 False\n",
      "307 True\n",
      "308 True\n",
      "309 True\n",
      "310 True\n",
      "311 False\n",
      "312 False\n",
      "313 False\n",
      "314 True\n",
      "315 True\n",
      "316 False\n",
      "317 False\n",
      "318 True\n",
      "319 True\n",
      "320 True\n",
      "321 True\n",
      "322 False\n",
      "323 False\n",
      "324 False\n",
      "325 False\n",
      "326 False\n",
      "327 False\n",
      "328 True\n",
      "329 False\n",
      "330 False\n",
      "331 True\n",
      "332 False\n",
      "333 False\n",
      "334 False\n",
      "335 False\n",
      "336 False\n",
      "337 True\n",
      "338 False\n",
      "339 True\n",
      "340 False\n",
      "341 True\n",
      "342 False\n",
      "343 True\n",
      "344 False\n",
      "345 True\n",
      "346 False\n",
      "347 False\n",
      "348 True\n",
      "349 True\n",
      "350 False\n",
      "351 True\n",
      "352 True\n",
      "353 False\n",
      "354 True\n",
      "355 False\n",
      "356 True\n",
      "357 False\n",
      "358 True\n",
      "359 True\n",
      "360 True\n",
      "361 False\n",
      "362 True\n",
      "363 True\n",
      "364 True\n",
      "365 True\n",
      "366 False\n",
      "367 False\n",
      "368 True\n",
      "369 True\n",
      "370 True\n",
      "371 True\n",
      "372 True\n",
      "373 False\n",
      "374 True\n",
      "375 True\n",
      "376 True\n",
      "377 False\n",
      "378 False\n",
      "379 False\n",
      "380 True\n",
      "381 True\n",
      "382 True\n",
      "383 True\n",
      "384 True\n",
      "385 True\n",
      "386 True\n",
      "387 False\n",
      "388 True\n",
      "389 True\n",
      "390 False\n",
      "391 True\n",
      "392 False\n",
      "393 True\n",
      "394 True\n",
      "395 True\n",
      "396 True\n",
      "397 True\n",
      "398 False\n",
      "399 True\n",
      "400 True\n",
      "401 True\n",
      "402 True\n",
      "403 True\n",
      "404 True\n",
      "405 True\n",
      "406 True\n",
      "407 True\n",
      "408 True\n",
      "409 True\n",
      "410 True\n",
      "411 False\n",
      "412 True\n",
      "413 True\n",
      "414 True\n",
      "415 True\n",
      "416 False\n",
      "417 True\n",
      "418 False\n",
      "419 False\n",
      "420 True\n",
      "421 True\n",
      "422 True\n",
      "423 True\n",
      "424 True\n",
      "425 True\n",
      "426 True\n",
      "427 True\n",
      "428 True\n",
      "429 True\n",
      "430 True\n",
      "431 True\n",
      "432 True\n",
      "433 True\n",
      "434 True\n",
      "435 True\n",
      "436 True\n",
      "437 True\n",
      "438 True\n",
      "439 True\n",
      "440 True\n",
      "441 True\n",
      "442 True\n",
      "443 True\n",
      "444 True\n",
      "445 True\n",
      "446 True\n",
      "447 True\n",
      "448 True\n",
      "449 False\n",
      "450 True\n",
      "451 True\n",
      "452 True\n",
      "453 True\n",
      "454 True\n",
      "455 True\n",
      "456 True\n",
      "457 True\n",
      "458 True\n",
      "459 True\n",
      "460 True\n",
      "461 True\n",
      "462 True\n",
      "463 True\n",
      "464 True\n",
      "465 True\n",
      "466 True\n",
      "467 True\n",
      "468 True\n",
      "469 False\n",
      "470 True\n",
      "471 True\n",
      "472 True\n",
      "473 True\n",
      "474 True\n",
      "475 True\n",
      "476 True\n",
      "477 True\n",
      "478 True\n",
      "479 True\n",
      "480 True\n",
      "481 True\n",
      "482 True\n",
      "483 True\n",
      "484 True\n",
      "485 True\n",
      "486 True\n",
      "487 True\n",
      "488 True\n",
      "489 True\n",
      "490 True\n",
      "491 True\n",
      "492 False\n",
      "493 True\n",
      "494 True\n",
      "495 True\n",
      "496 True\n",
      "497 True\n",
      "498 True\n",
      "499 True\n",
      "500 False\n",
      "501 True\n",
      "502 True\n",
      "503 True\n",
      "504 True\n",
      "505 True\n",
      "506 False\n",
      "507 True\n",
      "508 True\n",
      "509 True\n",
      "510 True\n",
      "511 True\n",
      "512 True\n",
      "513 True\n",
      "514 True\n",
      "515 True\n",
      "516 True\n",
      "517 True\n",
      "518 True\n",
      "519 True\n",
      "520 True\n",
      "521 True\n",
      "522 True\n",
      "523 False\n",
      "524 True\n",
      "525 True\n",
      "526 True\n",
      "527 True\n",
      "528 True\n",
      "529 True\n",
      "530 True\n",
      "531 True\n",
      "532 True\n",
      "533 True\n",
      "534 True\n",
      "535 True\n",
      "536 True\n",
      "537 True\n",
      "538 True\n",
      "539 True\n",
      "540 False\n",
      "541 True\n",
      "542 True\n",
      "543 True\n",
      "544 True\n",
      "545 True\n",
      "546 False\n",
      "547 True\n",
      "548 True\n",
      "549 True\n",
      "550 True\n",
      "551 True\n",
      "552 True\n",
      "553 True\n",
      "554 True\n",
      "555 True\n",
      "556 True\n",
      "557 True\n",
      "558 True\n",
      "559 True\n",
      "560 True\n",
      "561 True\n",
      "562 True\n",
      "563 True\n",
      "564 True\n",
      "565 True\n",
      "566 True\n",
      "567 True\n",
      "568 True\n",
      "569 True\n",
      "570 True\n",
      "571 True\n",
      "572 True\n",
      "573 True\n",
      "574 True\n",
      "575 True\n",
      "576 True\n",
      "577 True\n",
      "578 True\n",
      "579 True\n",
      "580 True\n",
      "581 True\n",
      "582 True\n",
      "583 True\n",
      "584 True\n",
      "585 True\n",
      "586 True\n",
      "587 True\n",
      "588 True\n",
      "589 True\n",
      "590 True\n",
      "591 True\n",
      "592 True\n",
      "593 True\n",
      "594 True\n",
      "595 True\n",
      "596 True\n",
      "597 True\n",
      "598 True\n",
      "599 True\n",
      "600 True\n",
      "601 True\n",
      "602 True\n",
      "603 False\n",
      "604 True\n",
      "605 True\n",
      "606 False\n",
      "607 True\n",
      "608 False\n",
      "609 False\n",
      "610 True\n",
      "611 True\n",
      "612 True\n",
      "613 False\n",
      "614 True\n",
      "615 True\n",
      "616 False\n",
      "617 False\n",
      "618 True\n",
      "619 False\n",
      "620 False\n",
      "621 True\n",
      "622 False\n",
      "623 True\n",
      "624 False\n",
      "625 True\n",
      "626 True\n",
      "627 True\n",
      "628 False\n",
      "629 True\n",
      "630 True\n",
      "631 True\n",
      "632 True\n",
      "633 True\n",
      "634 False\n",
      "635 False\n",
      "636 False\n",
      "637 False\n",
      "638 False\n",
      "639 False\n",
      "640 True\n",
      "641 False\n",
      "642 True\n",
      "643 True\n",
      "644 True\n",
      "645 True\n",
      "646 False\n",
      "647 False\n",
      "648 True\n",
      "649 True\n",
      "650 True\n",
      "651 True\n",
      "652 False\n",
      "653 True\n",
      "654 False\n",
      "655 True\n",
      "656 False\n",
      "657 True\n",
      "658 True\n",
      "659 False\n",
      "660 True\n",
      "661 True\n",
      "662 True\n",
      "663 True\n",
      "664 False\n",
      "665 False\n",
      "666 True\n",
      "667 False\n",
      "668 False\n",
      "669 True\n",
      "670 False\n",
      "671 False\n",
      "672 True\n",
      "673 True\n",
      "674 True\n",
      "675 True\n",
      "676 False\n",
      "677 True\n",
      "678 True\n",
      "679 False\n",
      "680 True\n",
      "681 True\n",
      "682 True\n",
      "683 False\n",
      "684 False\n",
      "685 False\n",
      "686 False\n",
      "687 False\n",
      "688 False\n",
      "689 True\n",
      "690 True\n",
      "691 False\n",
      "692 True\n",
      "693 True\n",
      "694 False\n",
      "695 True\n",
      "696 True\n",
      "697 False\n",
      "698 True\n",
      "699 True\n",
      "700 True\n",
      "701 True\n",
      "702 False\n",
      "703 False\n",
      "704 True\n",
      "705 True\n",
      "706 False\n",
      "707 True\n",
      "708 True\n",
      "709 False\n",
      "710 False\n",
      "711 False\n",
      "712 True\n",
      "713 True\n",
      "714 False\n",
      "715 True\n",
      "716 True\n",
      "717 True\n",
      "718 True\n",
      "719 False\n",
      "720 False\n",
      "721 True\n",
      "722 False\n",
      "723 True\n",
      "724 True\n",
      "725 False\n",
      "726 True\n",
      "727 False\n",
      "728 True\n",
      "729 True\n",
      "730 True\n",
      "731 True\n",
      "732 False\n",
      "733 True\n",
      "734 False\n",
      "735 False\n",
      "736 True\n",
      "737 False\n",
      "738 False\n",
      "739 False\n",
      "740 True\n",
      "741 False\n",
      "742 True\n",
      "743 True\n",
      "744 True\n",
      "745 False\n",
      "746 True\n",
      "747 False\n",
      "748 True\n",
      "749 True\n",
      "750 True\n",
      "751 True\n",
      "752 True\n",
      "753 False\n",
      "754 False\n",
      "755 False\n",
      "756 True\n",
      "757 True\n",
      "758 False\n",
      "759 False\n",
      "760 False\n",
      "761 False\n",
      "762 True\n",
      "763 True\n",
      "764 False\n",
      "765 True\n",
      "766 False\n",
      "767 True\n",
      "768 True\n",
      "769 True\n",
      "770 True\n",
      "771 True\n",
      "772 False\n",
      "773 False\n",
      "774 True\n",
      "775 False\n",
      "776 True\n",
      "777 True\n",
      "778 True\n",
      "779 False\n",
      "780 False\n",
      "781 True\n",
      "782 False\n",
      "783 True\n",
      "784 True\n",
      "785 False\n",
      "786 False\n",
      "787 False\n",
      "788 True\n",
      "789 True\n",
      "790 False\n",
      "791 True\n",
      "792 True\n",
      "793 False\n",
      "794 True\n",
      "795 True\n",
      "796 True\n",
      "797 True\n",
      "798 True\n",
      "799 False\n",
      "800 True\n",
      "801 False\n",
      "802 True\n",
      "803 False\n",
      "804 True\n",
      "805 True\n",
      "806 True\n",
      "807 True\n",
      "808 True\n",
      "809 True\n",
      "810 True\n",
      "811 True\n",
      "812 True\n",
      "813 True\n",
      "814 True\n",
      "815 True\n",
      "816 True\n",
      "817 True\n",
      "818 True\n",
      "819 True\n",
      "820 True\n",
      "821 True\n",
      "822 True\n",
      "823 True\n",
      "824 True\n",
      "825 False\n",
      "826 True\n",
      "827 True\n",
      "828 True\n",
      "829 True\n",
      "830 True\n",
      "831 True\n",
      "832 True\n",
      "833 True\n",
      "834 True\n",
      "835 True\n",
      "836 True\n",
      "837 True\n",
      "838 True\n",
      "839 True\n",
      "840 True\n",
      "841 True\n",
      "842 True\n",
      "843 True\n",
      "844 True\n",
      "845 False\n",
      "846 False\n",
      "847 True\n",
      "848 True\n",
      "849 True\n",
      "850 True\n",
      "851 True\n",
      "852 True\n",
      "853 True\n",
      "854 False\n",
      "855 True\n",
      "856 True\n",
      "857 True\n",
      "858 True\n",
      "859 True\n",
      "860 True\n",
      "861 True\n",
      "862 True\n",
      "863 True\n",
      "864 True\n",
      "865 True\n",
      "866 True\n",
      "867 True\n",
      "868 True\n",
      "869 True\n",
      "870 True\n",
      "871 True\n",
      "872 True\n",
      "873 True\n",
      "874 True\n",
      "875 True\n",
      "876 True\n",
      "877 True\n",
      "878 True\n",
      "879 True\n",
      "880 True\n",
      "881 True\n",
      "882 False\n",
      "883 True\n",
      "884 False\n",
      "885 True\n",
      "886 True\n",
      "887 True\n",
      "888 True\n",
      "889 True\n",
      "890 True\n",
      "891 True\n",
      "892 True\n",
      "893 True\n",
      "894 True\n",
      "895 True\n",
      "896 True\n",
      "897 True\n",
      "898 True\n",
      "899 True\n",
      "900 True\n",
      "901 True\n",
      "902 True\n",
      "903 True\n",
      "904 True\n",
      "905 True\n",
      "906 True\n",
      "907 True\n",
      "908 True\n",
      "909 True\n",
      "910 True\n",
      "911 True\n",
      "912 False\n",
      "913 True\n",
      "914 True\n",
      "915 False\n",
      "916 True\n",
      "917 True\n",
      "918 True\n",
      "919 True\n",
      "920 True\n",
      "921 True\n",
      "922 True\n",
      "923 True\n",
      "924 True\n",
      "925 True\n",
      "926 True\n",
      "927 True\n",
      "928 True\n",
      "929 True\n",
      "930 True\n",
      "931 True\n",
      "932 True\n",
      "933 True\n",
      "934 True\n",
      "935 True\n",
      "936 True\n",
      "937 True\n",
      "938 True\n",
      "939 True\n",
      "940 True\n",
      "941 True\n",
      "942 True\n",
      "943 True\n",
      "944 True\n",
      "945 True\n",
      "946 True\n",
      "947 True\n",
      "948 True\n",
      "949 True\n",
      "950 True\n",
      "951 True\n",
      "952 True\n",
      "953 True\n",
      "954 True\n",
      "955 True\n",
      "956 True\n",
      "957 True\n",
      "958 True\n",
      "959 True\n",
      "960 True\n",
      "961 True\n",
      "962 True\n",
      "963 True\n",
      "964 True\n",
      "965 True\n",
      "966 True\n",
      "967 True\n",
      "968 True\n",
      "969 True\n",
      "970 True\n",
      "971 True\n",
      "972 True\n",
      "973 True\n",
      "974 True\n",
      "975 True\n",
      "976 True\n",
      "977 True\n",
      "978 True\n",
      "979 True\n",
      "980 False\n",
      "981 True\n",
      "982 True\n",
      "983 True\n",
      "984 True\n",
      "985 True\n",
      "986 True\n",
      "987 True\n",
      "988 True\n",
      "989 True\n",
      "990 True\n",
      "991 True\n",
      "992 True\n",
      "993 True\n",
      "994 True\n",
      "995 True\n",
      "996 True\n",
      "997 True\n",
      "998 True\n",
      "999 True\n"
     ]
    }
   ],
   "source": [
    "mismatches = []\n",
    "confusion_m = np.zeros((5,5))\n",
    "\n",
    "\n",
    "for i,ex in enumerate(test_data):\n",
    "    pos,features,labels = list(zip(*ex['points']))\n",
    "    predicted = int(get_class(features,pos).argmax())\n",
    "    match = (ex['class'] == predicted)\n",
    "    confusion_m[ex['class'],predicted] +=1\n",
    "    if not match:\n",
    "        mismatches.append(i)\n",
    "    print(i,match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/miniconda3/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def calcDist(p1,p2):\n",
    "    x1,y1 = p1\n",
    "    x2,y2 = p2\n",
    "    dist = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "    return dist\n",
    "\n",
    "mismatches = []\n",
    "def get_score(p1,p2,p3,c,s1,s2,s3):\n",
    "    ab = calcDist(p1,p2)\n",
    "    bc = calcDist(p2,p3)\n",
    "    ca = calcDist(p3,p1)\n",
    "    max_d = np.max([ab,bc,ca])\n",
    "    abn,bcn,can = ab/max_d,bc/max_d,ca/max_d\n",
    "    \n",
    "    mismatch = (1-(abn - c[0]))*s1*s2 + (1-(bcn - c[1]))*s2*s3 \n",
    "    if len(c)==3:\n",
    "        mismatch += (1-(can - c[2]))*s3*s1\n",
    "    mismatches.append(mismatch)\n",
    "    if mismatch < 1:\n",
    "        return 0\n",
    "    else: return mismatch\n",
    "\n",
    "def get_best_score(candidates,c):\n",
    "    # rewrite for better effectivity\n",
    "    old_score = 0\n",
    "    for i1,_ in enumerate(candidates[0][0]):\n",
    "        s1,p1 = candidates[0][0][i1],candidates[0][1][i1]\n",
    "        for i2,_ in enumerate(candidates[1][0]):\n",
    "            s2,p2 = candidates[1][0][i2],candidates[1][1][i2]\n",
    "            for i3,_ in enumerate(candidates[2][0]):\n",
    "                s3,p3 = candidates[2][0][i3],candidates[2][1][i3]\n",
    "                score = get_score(p1,p2,p3,c,s1,s2,s3)\n",
    "                if score > old_score:\n",
    "                    old_score = score\n",
    "    return old_score\n",
    "\n",
    "\n",
    "def get_score_for_constraint(c):\n",
    "    candidates = choosed_points_np[c[0]]\n",
    "    return get_best_score(candidates,c[1])\n",
    "\n",
    "best_c_scores = torch.zeros(len(filtered_const))\n",
    "for i,v in enumerate(filtered_const):\n",
    "    if v > 0:\n",
    "        best_c_scores[i] = get_score_for_constraint(all_c[i])\n",
    "best_c_scores_masked = cons_score.masked_fill(best_c_scores < t4,0)\n",
    "clas_score_2 = torch.mv(const_to_class,best_c_scores_masked)\n",
    "clas_score_2_masked = (clas_score_2 > t5).float()\n",
    "clas_score_2_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(mismatches)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.hist(mismatches, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2480, 1.3334, 1.0669, 1.9293, 1.6127, 1.6831, 1.4751, 1.2649, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_c_scores\n",
    "#clas_score_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[0.8421, 0.7489, 0.3651,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.8584, 0.7035, 0.4028,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.4829, 0.4375, 0.3959,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.5718, 0.4075, 0.3723,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.3092, 0.3061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.4621, 0.3586, 0.3549,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       "indices=tensor([[  1,   0,  37,  ...,  35,  36, 106],\n",
       "        [  1,   0,  84,  ...,  36,  38, 106],\n",
       "        [ 29,  84,   1,  ...,  37,  38, 106],\n",
       "        ...,\n",
       "        [  6,  27,  57,  ...,  39,  40, 106],\n",
       "        [ 24,  55,  73,  ...,  37,  38, 106],\n",
       "        [ 55,  77, 100,  ...,  37,  38, 106]]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_masked.sort(dim=1,descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
